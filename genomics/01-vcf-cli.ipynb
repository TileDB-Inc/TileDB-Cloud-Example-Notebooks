{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "TileDB-VCF offers several interfaces to create and query your genomic variant data. Here we'll utilize the command-line interface (CLI) to walk through the process of loading raw VCF data into TileDB and then performing a few different types of exports.\n",
    "\n",
    "***Hint: By the way, you can launch an interactive version of this tutoral using [TileDB Cloud Notebooks](https://console.tiledb.com/notebooks).***\n",
    "\n",
    "\n",
    "# Create a Dataset\n",
    "\n",
    "The process of creating a new VCF dataset involves 3 phases: `create`, `register`, and `store`. Detailed information about each phase is provided in the [ingestion algorithm](https://docs.tiledb.com/genomics/advanced/ingestion-algorithm) section.\n",
    "\n",
    "\n",
    "We'll start with a small example using 3 synthetic VCF files available in `data/vcfs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "├── s3-bcf-samples.txt\n",
      "└── vcfs\n",
      "    ├── G1.vcf.gz\n",
      "    ├── G1.vcf.gz.csi\n",
      "    ├── G2.vcf.gz\n",
      "    ├── G2.vcf.gz.csi\n",
      "    ├── G3.vcf.gz\n",
      "    └── G3.vcf.gz.csi\n",
      "\n",
      "1 directory, 7 files\n"
     ]
    }
   ],
   "source": [
    "tree data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index files are required for ingestion. If your VCF/BCF files have not been indexed you can use [`bcftools`](https://samtools.github.io/bcftools/bcftools.html) to do so:\n",
    "\n",
    "```shell\n",
    "for f in data/vcfs/*.vcf.gz; do bcftools index -c $f; done\n",
    "```\n",
    "\n",
    "Creating an empty dataset is the first step to ingesting your VCF data. Let's save the dataset in a new directory called `small_dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiledbvcf create --uri small_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to **register** the samples that will be ingested using either a text file that contains the location of each VCF or by directly passing a list of VCF files to the command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiledbvcf register --uri small_dataset data/vcfs/G*.vcf.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can always determine which samples have been registered with a dataset using the `list` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G1\n",
      "G2\n",
      "G3\n"
     ]
    }
   ],
   "source": [
    "tiledbvcf list --uri small_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to actually ingest the VCF files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiledbvcf store --uri small_dataset data/vcfs/G*.vcf.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Let's verify evertyhing went okay using the `stat` command to provide high-level statistics about our dataset including the number of samples it contains and the variant attributes it includes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for dataset 'small_dataset':\n",
      "- Version: 2\n",
      "- Row tile extent: 10\n",
      "- Tile capacity: 10,000\n",
      "- Anchor gap: 1,000\n",
      "- Number of registered samples: 3\n",
      "- Extracted attributes: none\n"
     ]
    }
   ],
   "source": [
    "tiledbvcf stat --uri small_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you have succesfully created and populated a TileDB VCF dataset. By default *all* metadata recorded in the VCF data lines are ingested but you can override this behavior with the `--attributes` flag, which allows you to specify the subset of fields you want to include.\n",
    "\n",
    "\n",
    "\n",
    "# Incremental Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key advantages to using TileDB as the data store for your variant data is the ability to efficiently add new samples as they become available. Furthermore, TileDB's native cloud features makes it possible to ingest samples directly from remote locations. Here, we'll register and ingest the following additional samples, which are located on [AWS S3](https://aws.amazon.com/s3/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G4.bcf\n",
      "s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G5.bcf\n",
      "s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G6.bcf\n",
      "s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G7.bcf\n",
      "s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G8.bcf\n",
      "s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G9.bcf\n",
      "s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G10.bcf\n"
     ]
    }
   ],
   "source": [
    "cat data/s3-bcf-samples.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: Samples in this second batch are stored as `BCF` files which are also supported by TileDB-VCF.*\n",
    "\n",
    "This process is identical to the steps perfomed above, the only changes needed to our code involve setting `--scratch-mb` to allocate some temporary space for downloading the files and providing the URLs for the remote files. In this case, we'll simply pass the `s3-bcf-samples.txt` file, which includes a list of the BCF files we want to register and ingest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiledbvcf register \\\n",
    "  --uri small_dataset \\\n",
    "  --scratch-mb 1 \\\n",
    "  --samples-file data/s3-bcf-samples.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: When ingesting samples from S3, you must configure enough disk scratch space to hold at least 20 samples (in general, 2 &times; `row_tile_extent` samples).*\n",
    "\n",
    "You can add the `--verbose` flag to print out more information during the `store` phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization completed in 21.3871 sec.\n",
      "Finished ingesting 0 / 7 samples (1.17e-05 sec)...\n",
      "Done. Ingested 1,391 records (+ 69,548 anchors) from 7 samples in 30.4475 seconds.\n"
     ]
    }
   ],
   "source": [
    "tiledbvcf store \\\n",
    "  --uri small_dataset \\\n",
    "  --scratch-mb 10 \\\n",
    "  --samples-file data/s3-bcf-samples.txt \\\n",
    "  --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's run the `stat` command to verify our dataset now includes 10 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for dataset 'small_dataset':\n",
      "- Version: 2\n",
      "- Row tile extent: 10\n",
      "- Tile capacity: 10,000\n",
      "- Anchor gap: 1,000\n",
      "- Number of registered samples: 10\n",
      "- Extracted attributes: none\n"
     ]
    }
   ],
   "source": [
    "tiledbvcf stat --uri small_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you've updated an existing dataset, which inititally contained only *local VCF files*, by adding new *BCF files* located on a *remote* server. Because TileDB is designed to be easily updatable, this process happens efficiently and without ever touching the previously ingested data—avoiding computationally expensive operations like regenerating combined VCF files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying and Exporting\n",
    "\n",
    "Now that you've created a TileDB-VCF dataset, you can begin to query the variant data for the 10 ingested samples. As mentioned earlier, several APIs are available for accessing the data. Notebook 2 provides examples using the Python module. Here, we'll continue working with the CLI, focusing now on the `export` command, which can produce 3 different types of outputs for any given query:\n",
    "\n",
    "1. VCF (or BCF), producing one `.vcf` output file per exported sample\n",
    "2. Tabular, producing a single tab-separated value (TSV) text file containing all intersecting records across the exported samples\n",
    "3. Count-only, outputs a count of the total number of intersecting records (no output file is produced)\n",
    "\n",
    "The following examples demonstrate basic usage of the CLI for export; see the `tiledbvcf export --help` or the [online documentation](https://docs.tiledb.com/genomics/apis/cli) for more information.\n",
    "\n",
    "## Export VCF/BCFs\n",
    "\n",
    "In this example we will export several genomic regions from `small_dataset` created above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized TileDB query with 3 column ranges.\n",
      "Processed 315 cells in 6.62e-05 sec. Reported 13 cells.\n",
      "Done. Exported 13 records in 15.2308 seconds.\n"
     ]
    }
   ],
   "source": [
    "mkdir -p data/exported-subsets\n",
    "\n",
    "tiledbvcf export \\\n",
    "  --uri small_dataset \\\n",
    "  --regions 1:1-50000,2:1-50000,3:1-50000 \\\n",
    "  --sample-names G1,G2,G3 \\\n",
    "  --output-dir data/exported-subsets \\\n",
    "  --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produced the three `bcf` files shown below, each of which contains the records intersecting the specified regions for the corresponding sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/exported-subsets\n",
      "├── G1.bcf\n",
      "├── G2.bcf\n",
      "└── G3.bcf\n",
      "\n",
      "0 directories, 3 files\n"
     ]
    }
   ],
   "source": [
    "tree data/exported-subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `bcftools` to examine any of the exported `bcf` files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t13350\t.\tA\t<NON_REF>\t.\t.\tEND=36258\tGT:DP:GQ:MIN_DP:PL\t1/0:50:3:43:44,29,99\n",
      "1\t42091\t.\tA\t<NON_REF>\t.\t.\tEND=101445\tGT:DP:GQ:MIN_DP:PL\t0/0:8:91:60:35,62,92\n",
      "2\t11625\t.\tT\t<NON_REF>\t.\t.\tEND=106375\tGT:DP:GQ:MIN_DP:PL\t0/0:27:72:76:70,30,83\n",
      "3\t14580\t.\tT\t<NON_REF>\t.\t.\tEND=86190\tGT:DP:GQ:MIN_DP:PL\t0/1:50:78:41:67,11,43\n"
     ]
    }
   ],
   "source": [
    "bcftools view --no-header data/exported-subsets/G1.bcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a sample does not contain any intersecting records an output file is still created but it will include 0 records. \n",
    "\n",
    "In order to export *all* records, simply omit the `--regions` argument. For example, to recover the original BCFs for two samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized TileDB query with 1 column ranges.\n",
      "Processed 20758 cells in 0.0010847 sec. Reported 412 cells.\n",
      "Done. Exported 412 records in 2.61805 seconds.\n"
     ]
    }
   ],
   "source": [
    "mkdir -p data/exported-full\n",
    "\n",
    "tiledbvcf export \\\n",
    "  --uri small_dataset \\\n",
    "  --sample-names G4,G5 \\\n",
    "  --output-dir data/exported-full \\\n",
    "  --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will output two BCFs for the corresponding samples that include the complete set of information from the original files. Note, while these exports are *lossless* in terms of the actual stored data, they may not be *identical* to the original files. For example, exported BCF/VCFs will always include an `END` position field even if one was not present in the original files, and the ordering of fields within the `INFO` and `FORMAT` columns may also differ. \n",
    "\n",
    "## Export to TSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll look at exporting data from `small_dataset` in *tabular* format, which makes it convenient to load into other tools for downstream analyses. The command arguments are largely the same as for exporting to VCF/BCF, you just need to change the output type to `t` (short for TSV) and specify which VCF fields should be included as columns in the output table. The standard fields include:\n",
    "\n",
    "- `SAMPLE` (always included)\n",
    "- `ID`\n",
    "- `REF`\n",
    "- `ALT`\n",
    "- `QUAL`\n",
    "- `CHR`\n",
    "- `POS`\n",
    "- `FILTER`\n",
    "\n",
    "Fields within the `FORMAT` and `INFO` columns are accessed using special prefixes, `S:` and `I:`, respectively. This notation is used below to include genotype information from the `FORMAT:GT` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE\tCHR\tPOS\tI:END\tREF\tS:GT\n",
      "G4\t1\t10485\t40554\tG\t0,1\n",
      "G3\t1\t11758\t92302\tT\t1,1\n",
      "G2\t1\t12410\t50341\tA\t1,1\n",
      "G1\t1\t13350\t36258\tA\t1,0\n",
      "G1\t1\t42091\t101445\tA\t0,0\n",
      "G4\t1\t46864\t86622\tG\t1,0\n"
     ]
    }
   ],
   "source": [
    "tiledbvcf export \\\n",
    "    --uri small_dataset \\\n",
    "    -Ot --tsv-fields CHR,POS,I:END,REF,S:GT \\\n",
    "    -s G1,G2,G3,G4 \\\n",
    "    --regions 1:1-50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are output in a long form table where each row represents a single variant that overlapped one of the specified regions for a particular sample. \n",
    "\n",
    "There's one more special prefix worth pointing out: `Q:` (for *query*), which allows you to include columns containing the start (`Q:POS`) and end (`Q:END`) positions for the query region in which the variant is located. Here we perform the same query across a few more regions, add the query columns, and save the output to `data/exported-regions.tsv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE\tQ:POS\tQ:END\tCHR\tPOS\tI:END\tREF\tS:GT\n",
      "G4\t1\t50000\t1\t10485\t40554\tG\t0,1\n",
      "G3\t1\t50000\t1\t11758\t92302\tT\t1,1\n",
      "G2\t1\t50000\t1\t12410\t50341\tA\t1,1\n",
      "G1\t1\t50000\t1\t13350\t36258\tA\t1,0\n",
      "G1\t1\t50000\t1\t42091\t101445\tA\t0,0\n",
      "G4\t1\t50000\t1\t46864\t86622\tG\t1,0\n",
      "G3\t60000\t100000\t2\t11431\t93855\tA\t1,0\n",
      "G1\t60000\t100000\t2\t11625\t106375\tT\t0,0\n",
      "G2\t60000\t100000\t2\t14907\t92383\tT\t1,1\n",
      "G4\t60000\t100000\t2\t62758\t132118\tA\t1,1\n",
      "G2\t60000\t100000\t2\t92670\t145136\tC\t0,1\n",
      "G3\t60000\t100000\t2\t93967\t149850\tC\t1,0\n",
      "G2\t110000\t160000\t3\t78418\t148323\tG\t0,0\n",
      "G4\t110000\t160000\t3\t91891\t169481\tA\t1,0\n",
      "G3\t110000\t160000\t3\t111926\t162835\tA\t0,0\n",
      "G1\t110000\t160000\t3\t112593\t174752\tT\t1,0\n"
     ]
    }
   ],
   "source": [
    "tiledbvcf export \\\n",
    "    --uri small_dataset \\\n",
    "    -Ot --tsv-fields Q:POS,Q:END,CHR,POS,I:END,REF,S:GT \\\n",
    "    -s G1,G2,G3,G4 \\\n",
    "    --regions 1:1-50000,2:60000-100000,3:110000-160000 \\\n",
    "    --output-path data/exported-regions.tsv\n",
    "\n",
    "cat data/exported-regions.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Fri Jun  5 19:41:05 UTC 2020\n"
     ]
    }
   ],
   "source": [
    "echo \"Updated: $(date)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TileDB-VCF build 984aeda-modified\n",
      "TileDB version 1.7.7\n"
     ]
    }
   ],
   "source": [
    "tiledbvcf --version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
