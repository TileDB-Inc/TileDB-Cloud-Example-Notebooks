{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bigger-irish",
   "metadata": {},
   "source": [
    "# 1000 Genomes Project TileDB-VCF Example\n",
    "\n",
    "TileDB Cloud's Public Data Explorer provides access to a complete version of the [1000 Genomes Project](https://www.internationalgenome.org/) Phase 3 data stored as a TileDB-VCF Dataset. This tutorial provides a walkthrough of the steps we followed to create this resource using [TileDB-VCF](https://github.com/TileDB-Inc/TileDB-VCF.git)'s Python package. \n",
    "\n",
    "This tutorial covers:\n",
    "\n",
    "* pre-processing combined project VCF files (pVCF) into single sample VCF files appropriate for TileDB-VCF\n",
    "* creating TileDB-VCF datasets by ingesting remote VCF files stored on S3\n",
    "* querying TileDB-VCF datasets via Python and accessing the results as Pandas `Dataframes`\n",
    "* exporting VCF files from a TileDB-VCF dataset using the command line interface (CLI)\n",
    "* scaling ingestion and query operations utilizing serverless user-defined functions (UDFs)\n",
    "\n",
    "Please see the [official documentation](https://docs.tiledb.com/solutions/integrations/population-genomics) for more comprehensive usage details, API references, as well as instructions for installing TileDB-VCF.\n",
    "\n",
    "## Setup and Requirements\n",
    "\n",
    "Following along with this tutorial requires:\n",
    "\n",
    "**System tools**\n",
    "- `tiledbvcf` CLI\n",
    "- `aws` command line tool\n",
    "- `bcftools`\n",
    "- `parallel`\n",
    "\n",
    "**Python packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import tiledbvcf\n",
    "from envbash import load_envbash\n",
    "\n",
    "tiledbvcf.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-shuttle",
   "metadata": {},
   "source": [
    "The following is a custom function to simplify the process of obtaining URIs for files located on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "prepared-jordan",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "List URIs for files in an S3 bucket\n",
    ":param bucket (string) Bucket name to list.\n",
    ":param prefix (string) Limits the response to keys that begin with the specified prefix\n",
    ":param suffix (string, optional) Limits the response to files with the specified extension\n",
    "\"\"\"\n",
    "def aws_s3_ls(bucket, prefix, suffix = None):\n",
    "    s3 = boto3.client('s3')\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "    response_files = response[\"Contents\"]\n",
    "    list_incomplete = response[\"IsTruncated\"]\n",
    "\n",
    "    while list_incomplete:\n",
    "        response = s3.list_objects_v2(\n",
    "            Bucket=bucket, \n",
    "            Prefix=prefix,\n",
    "            ContinuationToken = response[\"NextContinuationToken\"]\n",
    "        )\n",
    "        response_files.extend(response[\"Contents\"])\n",
    "        list_incomplete = response[\"IsTruncated\"]\n",
    "\n",
    "    output = [f\"s3://{bucket}/{file['Key']}\" for file in response_files]\n",
    "\n",
    "    if suffix is not None:\n",
    "        output = [file for file in output if file.endswith(suffix)]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-extraction",
   "metadata": {},
   "source": [
    "We'll also define define a few variables that we'll refer to throughout the tutorial. You'll need to change some of the remote locations to create your own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "local-trinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_bucket=\"genomic-datasets\"\n",
    "array_prefix=\"notebooks/1kgp3/1kgp3-array\"\n",
    "array_uri = f\"s3://{aws_bucket}/{array_prefix}\"\n",
    "\n",
    "bcfs_prefix=\"1kg/1kgdbv5_supp/1kgdb_sample_bcfs\"\n",
    "bcf_uris = []\n",
    "\n",
    "bedfile = \"s3://genomic-datasets/notebooks/1kgp3/hg37_chr1_covidHgiGwasR4PvalC2_plog3.bed.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-trauma",
   "metadata": {},
   "source": [
    "## Preprocessing the Raw 1KG pVCF File\n",
    "\n",
    "Unlike the more modern [high-coverage version](https://www.internationalgenome.org/announcements/3202-samples-at-high-coverage-from-NYGC/) of the 1000 Genomes (1KG) data, which provides raw single-sample gVCF files, the low-coverage Phase 3 data only provides chromosome-specific pVCF files that combine variant calls for all 2,504 samples. In order to ingest this data into a TileDB-VCF dataset we must convert the densified pVCF files into sparse single-sample VCF files.\n",
    "\n",
    "*Note: We're only working with data from chromosome 1 for the purposes of this tutorial. However, the process of working with whole genome data is the same.*\n",
    "\n",
    "The original VCF file was downloaded from the AWS Open Data Registry: <a href=\"https://registry.opendata.aws/1000-genomes\" class=\"uri\">https://registry.opendata.aws/1000-genomes</a>.\n",
    "\n",
    "```sh\n",
    "aws s3 sync \\\n",
    "    --exclude \"*\" --include \"ALL.chr1.*\" \\\n",
    "    s3://1000genomes/release/20130502/ \\\n",
    "    data/1000genomes/\n",
    "```\n",
    "\n",
    "First, we use `bcftools` to split the pVCF file back into single-sample VCF files\n",
    "\n",
    "```sh\n",
    "bcftools +split \\\n",
    "    -Ob \\\n",
    "    -o data/split-bcfs \\\n",
    "    data/1000genomes/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\n",
    "```\n",
    "\n",
    "Next we’ll filter the split VCF files to include only records with a non-reference allele and remove `INFO` attributes that are either static (e.g., `NS`) or cohort-specific and recoverable (e.g., `AF`). We’ll save the final pre-processed files in *BCF* format, which is the binary representation of the VCF format.\n",
    "\n",
    "```sh\n",
    "rm_tags=INFO/AF,INFO/NS,INFO/EAS_AF,INFO/AMR_AF,INFO/AFR_AF,INFO/EUR_AF,INFO/SAS_AF\n",
    "\n",
    "ls data/split-bcfs/*.bcf | parallel -j16 \\\n",
    "    \"bcftools view --min-ac 1 -Ou -s {/.} {} | bcftools annotate -Ob --remove $rm_tags -o data/filtered-bcfs/{/}\"\n",
    "```\n",
    "\n",
    "The resulting filtered BCF files are a close approximation of the raw single sample VCF files typically stored for large population genomics projects. The last step is to index the newly created BCF files.\n",
    "\n",
    "```sh\n",
    "ls data/filtered-bcfs/*.bcf | parallel -j16 \"bcftools index {}\"\n",
    "```\n",
    "\n",
    "For convenience, we have stored the pre-processed genome-wide single-sample VCF files on S3: \n",
    "\n",
    "```sh\n",
    "aws s3 cp --recursive data/filtered-bcfs/ s3://{aws_bucket}/{bcfs_prefix}/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-baghdad",
   "metadata": {},
   "source": [
    "## Storing Data in TileDB VCF\n",
    "\n",
    "We'll switch to Python for the rest of this tutorial and use the `tiledbvcf` package to ingest and query the pre-processed 1KG variant data\n",
    "\n",
    "The following was run on a `m5.4xlarge` system with a 300GB EBS volume to handle the large number of VCF files. Note that TileDB is *highly-tunable* and while the defaults were chosen to provide a good balance between ingestion speed, read performance, and dataset size, they can be be tweaked to better suit a specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-snapshot",
   "metadata": {},
   "source": [
    "### Create the dataset\n",
    "\n",
    "You can create a TileDB VCF dataset anywhere that TileDB supports, this could be a local filesystem, S3, Azure, Google Cloud Storage, HDFS, and more. For this example we’ll create it on S3, the most common use case.\n",
    "\n",
    "We're opening the dataset in *write* mode, with `verbose = True` to receive information about the progress of the ingestion, and `stats = True` to get some insight into TileDB's performance after ingestion completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-option",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tiledbvcf.Dataset(array_uri, stats = True, verbose = True, mode = \"w\")\n",
    "ds.create_dataset(extra_attrs = [\"fmt_GT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-negotiation",
   "metadata": {},
   "source": [
    "#### Storing the data\n",
    "\n",
    "Storing VCF data in TileDB-VCF simply requires a list of the VCF/BCF file locations. In this case, we'll provide a list of S3 URIS pointing to the BCF files, which will allow us to ingest them directly from their remote location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "considerable-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcf_uris = aws_s3_ls(aws_bucket, bcfs_prefix, suffix = \"bcf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-equity",
   "metadata": {},
   "source": [
    "Finally, we'll run the following command to ingest the pre-processed BCF files into a new TileDB-VCF dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.ingest_samples(\n",
    "    sample_uris = bcf_uris,\n",
    "    threads = 14,\n",
    "    memory_budget_mb = 2048 * 2,\n",
    "    sample_batch_size = 20,\n",
    "    scratch_space_path = \"/mnt/data/tmp\",\n",
    "    scratch_space_size = 4000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-freedom",
   "metadata": {},
   "source": [
    "We can see from the verbose output provided the following summary printed at the end:\n",
    "\n",
    "```\n",
    "All finalize tasks successfully completed. Waited for 5.87609 sec.\n",
    "Done. Ingested 10,771,609,147 records (+ 38,528,651 anchors) from 2,504 samples in 24,546.9 seconds.\n",
    "```\n",
    "\n",
    "This indicates we’ve ingested over 10 billion records into the TileDB-VCF dataset in just under 7 hours. With an `m5.4xlarge` instance costing \\$0.768 an hour, the cost of ingestion was just over \\$5.00 USD.\n",
    "\n",
    "The final array is 70Gb, or just under half the size of the individual compressed BCF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "motivated-official",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== WRITE ====\n",
      "\n",
      "- Number of write queries: 70434\n",
      "\n",
      "- Number of attributes written: 633906\n",
      "  * Number of fixed-sized attributes written: 211302\n",
      "  * Number of var-sized attributes written: 422604\n",
      "- Number of dimensions written: 211302\n",
      "  * Number of fixed-sized dimensions written: 70434\n",
      "  * Number of var-sized dimensions written: 140868\n",
      "\n",
      "- Number of bytes written: 75362471867 bytes (70.1868 GB) \n",
      "- Number of write operations: 21863256\n",
      "- Number of bytes filtered: 2278398374656 bytes (2121.92 GB) \n",
      "- Filtering deflation factor: 30.2325x\n",
      "\n",
      "- Total metadata written: 89454680 bytes (0.0833112 GB) \n",
      "  * Array schema: 813 bytes (7.57165e-07 GB) \n",
      "  * Fragment metadata footer: 1605894 bytes (0.00149561 GB) \n",
      "  * R-tree: 7523989 bytes (0.00700726 GB) \n",
      "  * Fixed-sized tile offsets: 38844555 bytes (0.0361768 GB) \n",
      "  * Var-sized tile offsets: 28188665 bytes (0.0262527 GB) \n",
      "  * Var-sized tile sizes: 13290764 bytes (0.012378 GB) \n",
      "\n",
      "- Time to write array metadata: 0.156775 secs\n",
      "  * Array metadata size: 110 bytes (1.02445e-07 GB) \n",
      "\n",
      "- Number of logical cells written: 10810137798\n",
      "- Number of logical tiles written: 1082318\n",
      "  * Number of fixed-sized physical tiles written: 304927944048\n",
      "  * Number of var-sized physical tiles written: 1219711776192\n",
      "\n",
      "- Write time: 8278.06 secs\n",
      "  * Time to split the coordinates buffer: 0.0454959 secs\n",
      "  * Time to check out-of-bounds coordinates: 68.1267 secs\n",
      "  * Time to check coordinate duplicates: 0.0474146 secs\n",
      "  * Time to check global order: 159.101 secs\n",
      "  * Time to prepare tiles: 2707.89 secs\n",
      "  * Time to compute coordinate metadata (e.g., MBRs): 294.138 secs\n",
      "  * Time to filter tiles: 2885.64 secs\n",
      "  * Time to write tiles: 2193.43 secs\n",
      "  * Time to write fragment metadata: 2039.88 secs\n",
      "\n",
      "- Time to finalize write query: 18168.3 secs\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ds.tiledb_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-swaziland",
   "metadata": {},
   "source": [
    "Following ingestion, it may help performance to consolidate the metadata fragments, which is currently only possible using the CLI.\n",
    "\n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-seven",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tiledbvcf utils consolidate fragment_meta -u {array_uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-basis",
   "metadata": {},
   "source": [
    "## Reading, Analysis, and Exporting\n",
    "\n",
    "In this section we will walk through accessing the TileDB-VCF dataset and also exporting back to VCF and TSV. First, we must reopen the dataset in *read* mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "liquid-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tiledbvcf.Dataset(array_uri, stats = True, verbose = True, mode = \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-pepper",
   "metadata": {},
   "source": [
    "#### Reading into Pandas Dataframe\n",
    "\n",
    "Pandas is one of the most popular data science tools in python. TileDB VCF’s python API produces results directly as Pandas `DataFrame`s, making it easy to analyze the data and leverage any of Pandas’ builtin algorithms.\n",
    "\n",
    "Let’s run a typical query on the 1kg TileDB-VCF dataset we created above. We’ll retrieve all variants that overlap the gene *MTOR* on chr 1 for sample HG00096, along with a few attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dangerous-translation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_name</th>\n",
       "      <th>contig</th>\n",
       "      <th>pos_start</th>\n",
       "      <th>pos_end</th>\n",
       "      <th>alleles</th>\n",
       "      <th>fmt_GT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43337897</td>\n",
       "      <td>43337897</td>\n",
       "      <td>[A, G]</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43339092</td>\n",
       "      <td>43339092</td>\n",
       "      <td>[C, T]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43339203</td>\n",
       "      <td>43339203</td>\n",
       "      <td>[G, A]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43340776</td>\n",
       "      <td>43340776</td>\n",
       "      <td>[T, C]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43340779</td>\n",
       "      <td>43340779</td>\n",
       "      <td>[A, G]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43341662</td>\n",
       "      <td>43341662</td>\n",
       "      <td>[A, G]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43342021</td>\n",
       "      <td>43342021</td>\n",
       "      <td>[G, A]</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43343390</td>\n",
       "      <td>43343390</td>\n",
       "      <td>[A, G]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43344193</td>\n",
       "      <td>43344193</td>\n",
       "      <td>[G, A]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43344632</td>\n",
       "      <td>43344632</td>\n",
       "      <td>[T, A]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43346551</td>\n",
       "      <td>43346551</td>\n",
       "      <td>[A, G]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43347556</td>\n",
       "      <td>43347556</td>\n",
       "      <td>[C, G]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43347925</td>\n",
       "      <td>43347925</td>\n",
       "      <td>[C, T]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43348081</td>\n",
       "      <td>43348081</td>\n",
       "      <td>[G, T]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43348184</td>\n",
       "      <td>43348184</td>\n",
       "      <td>[A, T]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43350603</td>\n",
       "      <td>43350603</td>\n",
       "      <td>[C, T]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43351984</td>\n",
       "      <td>43351984</td>\n",
       "      <td>[A, G]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>43352685</td>\n",
       "      <td>43352685</td>\n",
       "      <td>[A, G]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_name contig  pos_start   pos_end alleles  fmt_GT\n",
       "0      HG00096      1   43337897  43337897  [A, G]  [1, 0]\n",
       "1      HG00096      1   43339092  43339092  [C, T]  [1, 1]\n",
       "2      HG00096      1   43339203  43339203  [G, A]  [1, 1]\n",
       "3      HG00096      1   43340776  43340776  [T, C]  [1, 1]\n",
       "4      HG00096      1   43340779  43340779  [A, G]  [1, 1]\n",
       "5      HG00096      1   43341662  43341662  [A, G]  [1, 1]\n",
       "6      HG00096      1   43342021  43342021  [G, A]  [1, 0]\n",
       "7      HG00096      1   43343390  43343390  [A, G]  [1, 1]\n",
       "8      HG00096      1   43344193  43344193  [G, A]  [1, 1]\n",
       "9      HG00096      1   43344632  43344632  [T, A]  [1, 1]\n",
       "10     HG00096      1   43346551  43346551  [A, G]  [1, 1]\n",
       "11     HG00096      1   43347556  43347556  [C, G]  [1, 1]\n",
       "12     HG00096      1   43347925  43347925  [C, T]  [1, 1]\n",
       "13     HG00096      1   43348081  43348081  [G, T]  [1, 1]\n",
       "14     HG00096      1   43348184  43348184  [A, T]  [1, 1]\n",
       "15     HG00096      1   43350603  43350603  [C, T]  [1, 1]\n",
       "16     HG00096      1   43351984  43351984  [A, G]  [1, 1]\n",
       "17     HG00096      1   43352685  43352685  [A, G]  [1, 1]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = tiledbvcf.ReadConfig(memory_budget_mb=8192)\n",
    "ds = tiledbvcf.Dataset(array_uri, stats = True, cfg = cfg)\n",
    "\n",
    "ds.read(\n",
    "    attrs = [\"sample_name\", \"contig\", \"pos_start\", \"pos_end\", \"alleles\", \"fmt_GT\"], \n",
    "    regions = [\"1:43337848-43352772\"],\n",
    "    samples = [\"HG00096\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-console",
   "metadata": {},
   "source": [
    "If the `Dataset` object was created with `stats = True` you can print out a variety of useful information about the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unsigned-yacht",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== READ ====\n",
      "\n",
      "- Number of read queries: 5\n",
      "- Number of attempts until results are found: 5\n",
      "\n",
      "- Number of attributes read: 6\n",
      "  * Number of fixed-sized attributes read: 2\n",
      "  * Number of var-sized attributes read: 4\n",
      "- Number of dimensions read: 7\n",
      "  * Number of fixed-sized dimensions read: 1\n",
      "  * Number of var-sized dimensions read: 6\n",
      "\n",
      "- Number of logical tiles overlapping the query: 255\n",
      "- Number of physical tiles read: 5865\n",
      "  * Number of physical fixed-sized tiles read: 765\n",
      "  * Number of physical var-sized tiles read: 5100\n",
      "- Number of cells read: 15048\n",
      "- Number of result cells: 5029\n",
      "- Percentage of useful cells read: 33.4197%\n",
      "\n",
      "- Number of bytes read: 1640652 bytes (0.00152798 GB) \n",
      "- Number of read operations: 4389\n",
      "- Number of bytes unfiltered: 5584858 bytes (0.0052013 GB) \n",
      "- Unfiltering inflation factor: 3.40405x\n",
      "\n",
      "- Time to compute estimated result size: 0.984495 secs\n",
      "  * Time to compute tile overlap: 1.0006 secs\n",
      "    > Time to compute relevant fragments: 0.00160453 secs\n",
      "    > Time to load relevant fragment R-trees: 0.990314 secs\n",
      "    > Time to compute relevant fragment tile overlap: 0.00850442 secs\n",
      "\n",
      "- Time to open array: 1.89867 secs\n",
      "  * Time to load array schema: 0.183111 secs\n",
      "  * Time to load consolidated fragment metadata: 0.180766 secs\n",
      "  * Time to load fragment metadata: 0.0194822 secs\n",
      "\n",
      "- Total metadata read: 4158730 bytes (0.00387312 GB) \n",
      "  * Array schema: 1626 bytes (1.51433e-06 GB) \n",
      "  * Consolidated fragment metadata: 4010848 bytes (0.00373539 GB) \n",
      "  * R-tree: 52064 bytes (4.84884e-05 GB) \n",
      "  * Fixed-sized tile offsets: 40464 bytes (3.7685e-05 GB) \n",
      "  * Var-sized tile offsets: 24864 bytes (2.31564e-05 GB) \n",
      "  * Var-sized tile sizes: 28864 bytes (2.68817e-05 GB) \n",
      "\n",
      "- Time to load array metadata: 0.141049 secs\n",
      "  * Array metadata size: 220 bytes (2.04891e-07 GB) \n",
      "\n",
      "- Time to initialize the read state: 0.0205483 secs\n",
      "\n",
      "- Read time: 1.8673 secs\n",
      "  * Time to compute next partition: 0.00147777 secs\n",
      "  * Time to compute result coordinates: 1.62305 secs\n",
      "    > Time to compute sparse result tiles: 0.00036612 secs\n",
      "    > Time to read coordinate tiles: 1.61069 secs\n",
      "    > Time to unfilter coordinate tiles: 0.00480338 secs\n",
      "    > Time to compute range result coordinates: 0.00430682 secs\n",
      "  * Time to compute sparse result cell slabs: 2.4647e-05 secs\n",
      "  * Time to copy result attribute values: 0.241714 secs\n",
      "    > Time to read attribute tiles: 0.239645 secs\n",
      "    > Time to unfilter attribute tiles: 0.00118809 secs\n",
      "    > Time to copy fixed-sized attribute values: 0.000293404 secs\n",
      "    > Time to copy var-sized attribute values: 0.000346146 secs\n",
      "  * Time to copy result coordinates: 0.000948717 secs\n",
      "    > Time to copy fixed-sized coordinates: 0.000133175 secs\n",
      "    > Time to copy var-sized coordinates: 0.000659871 secs\n",
      "\n",
      "- Total read query time (array open + init state + read): 1.88785 secs\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ds.tiledb_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-intersection",
   "metadata": {},
   "source": [
    "For production-sized queries that encompass large portions of the genome it's more convenient to provide bed files with the query regions. Here, we'll use a bed file on S3 that contains 1,040 regions on chr1 that show at least a moderate association with with SARS-CoV-2 infection susceptibility (data obtained from the [COVID-19 Host Genetics Initiative](https://www.covid19hg.org/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "divine-airplane",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_name</th>\n",
       "      <th>contig</th>\n",
       "      <th>pos_start</th>\n",
       "      <th>pos_end</th>\n",
       "      <th>alleles</th>\n",
       "      <th>info_DP</th>\n",
       "      <th>fmt_GT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HG00097</td>\n",
       "      <td>1</td>\n",
       "      <td>2265099</td>\n",
       "      <td>2265099</td>\n",
       "      <td>[C, T]</td>\n",
       "      <td>[16479]</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HG00099</td>\n",
       "      <td>1</td>\n",
       "      <td>2265099</td>\n",
       "      <td>2265099</td>\n",
       "      <td>[C, T]</td>\n",
       "      <td>[16479]</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HG00107</td>\n",
       "      <td>1</td>\n",
       "      <td>2265099</td>\n",
       "      <td>2265099</td>\n",
       "      <td>[C, T]</td>\n",
       "      <td>[16479]</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>1</td>\n",
       "      <td>2337537</td>\n",
       "      <td>2337537</td>\n",
       "      <td>[C, G]</td>\n",
       "      <td>[11721]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HG00097</td>\n",
       "      <td>1</td>\n",
       "      <td>2337537</td>\n",
       "      <td>2337537</td>\n",
       "      <td>[C, G]</td>\n",
       "      <td>[11721]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3470</th>\n",
       "      <td>HG00102</td>\n",
       "      <td>1</td>\n",
       "      <td>247999680</td>\n",
       "      <td>247999680</td>\n",
       "      <td>[G, A]</td>\n",
       "      <td>[19889]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3471</th>\n",
       "      <td>HG00103</td>\n",
       "      <td>1</td>\n",
       "      <td>247999680</td>\n",
       "      <td>247999680</td>\n",
       "      <td>[G, A]</td>\n",
       "      <td>[19889]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3472</th>\n",
       "      <td>HG00105</td>\n",
       "      <td>1</td>\n",
       "      <td>247999680</td>\n",
       "      <td>247999680</td>\n",
       "      <td>[G, A]</td>\n",
       "      <td>[19889]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3473</th>\n",
       "      <td>HG00106</td>\n",
       "      <td>1</td>\n",
       "      <td>247999680</td>\n",
       "      <td>247999680</td>\n",
       "      <td>[G, A]</td>\n",
       "      <td>[19889]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3474</th>\n",
       "      <td>HG00107</td>\n",
       "      <td>1</td>\n",
       "      <td>247999680</td>\n",
       "      <td>247999680</td>\n",
       "      <td>[G, A]</td>\n",
       "      <td>[19889]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3475 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sample_name contig  pos_start    pos_end alleles  info_DP  fmt_GT\n",
       "0        HG00097      1    2265099    2265099  [C, T]  [16479]  [0, 1]\n",
       "1        HG00099      1    2265099    2265099  [C, T]  [16479]  [0, 1]\n",
       "2        HG00107      1    2265099    2265099  [C, T]  [16479]  [1, 0]\n",
       "3        HG00096      1    2337537    2337537  [C, G]  [11721]  [1, 1]\n",
       "4        HG00097      1    2337537    2337537  [C, G]  [11721]  [1, 1]\n",
       "...          ...    ...        ...        ...     ...      ...     ...\n",
       "3470     HG00102      1  247999680  247999680  [G, A]  [19889]  [1, 1]\n",
       "3471     HG00103      1  247999680  247999680  [G, A]  [19889]  [1, 1]\n",
       "3472     HG00105      1  247999680  247999680  [G, A]  [19889]  [1, 1]\n",
       "3473     HG00106      1  247999680  247999680  [G, A]  [19889]  [1, 1]\n",
       "3474     HG00107      1  247999680  247999680  [G, A]  [19889]  [1, 1]\n",
       "\n",
       "[3475 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = tiledbvcf.Dataset(array_uri, stats = True, cfg = cfg)\n",
    "\n",
    "df = ds.read(\n",
    "    attrs = [\"sample_name\", \"contig\", \"pos_start\", \"pos_end\", \"alleles\", \"info_DP\", \"fmt_GT\"], \n",
    "    samples = ds.samples()[:10],\n",
    "    bed_file = bedfile\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-minute",
   "metadata": {},
   "source": [
    "This query completed in 1.9 secs for 10 samples.\n",
    "\n",
    "#### Filter Example\n",
    "\n",
    "Using the pandas dataframe returned from TileDB-VCF we can apply additional filters. For instance if we wanted to filter the above result on read depth (`info_DP`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.info_DP.apply(lambda x: x[0] > 5000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-virus",
   "metadata": {},
   "source": [
    "The python API supports a variety of advanced uses: batching, partitioning, Dask, and more. We are happy to follow-up with additional details beyond these initial examples.\n",
    "\n",
    "### CLI Exporting\n",
    "\n",
    "In addition to the python API it is also possible to export the dataset back into VCF format. This can be helpful in interoperating with legacy tools.\n",
    "\n",
    "#### Exporting to VCF\n",
    "\n",
    "When exporting to VCF you can specify any number of samples, and each will be exported to its own file in vcf, compressed vcf or bcf format depdning on what you set for `--output-format`.\n",
    "\n",
    "For example to export the entire sample for `HG00096`, `HG00097`, and `HG00099` you can run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-aruba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tiledbvcf export --uri {array_uri} \\\n",
    "    --output-format v \\\n",
    "    --sample-names HG00096,HG00097,HG00099 \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-positive",
   "metadata": {},
   "source": [
    "This produces 3 files: `HG00096.vcf`, `HG00097.vcf`, and `HG00099.vcf`.\n",
    "\n",
    "##### Filtering Exports\n",
    "\n",
    "You can also combine the use of regions (bed file or list of regions passed to cli) to filter the export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tiledbvcf export --uri {array_uri} \\\n",
    "    --output-format v \\\n",
    "    --sample-names HG00096,HG00097,HG00099 \\\n",
    "    --regions-file {bedfile} \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-acquisition",
   "metadata": {},
   "source": [
    "This will also produce the 3 VCF files, like the previous export. However, these files are filtered for the same SARS-CoV-2 associated genomic regions specified in the bed file.\n",
    "\n",
    "\n",
    "#### Exporting to TSV\n",
    "\n",
    "For even more generic usecases you can export data to tab seperate files with the `--output-format t` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tiledbvcf export --uri {array_uri} \\\n",
    "    --output-format t \\\n",
    "    --tsv-fields CHR,POS,I:END,REF,ALT,S:GT,Q:POS,Q:END \\\n",
    "    --sample-names HG00096,HG00097,HG00099 \\\n",
    "    --regions-file {bedfile} \\\n",
    "    --verbose --output-path sars-cov-2-associated-regions.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-modification",
   "metadata": {},
   "source": [
    "## Scaling with Severless Computer\n",
    "\n",
    "### UDF-based Ingestion\n",
    "\n",
    "Now that we've seen the basic mechanics of how to create and query a TileDB-VCF dataset, we'll look at some options for solving the most common pain point in modern genomics analyses: scaling operations to process the massive cohorts produced by modern sequencing projects.\n",
    "\n",
    "Out of the box, TileDB supports fully parallel reads and writes so queries can be partitioned based on genomic regions, samples, or both, and distributed across multiple cores, with each core processing a different chunk of the data. TileDB-VCF provides a number of options to leverage these scaling features, including integrations with frameworks like Dask for parallel computing in Python, as well as Apache Spark.\n",
    "\n",
    "Here, we'll utilize TileDB Cloud's serverless compute service to easily convert our previous single-node ingestion into one that's automatically processed by as many nodes as we have sample batches. First, we'll need to import `tiledb.cloud`, which provides the `Delayed` module we'll use to convert our query function into a user-defined function (UDF) that can be serialized and shipped to serverless node for processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-ukraine",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiledb.cloud.compute import Delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-validity",
   "metadata": {},
   "source": [
    "Then we'll need to create a callable function that performs our ingestion for a given batch of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "spectacular-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_bcf_files(array_uri, bcf_uris, cfg):\n",
    "    print(f\"Ingesting {len(bcf_uris)} starting with {bcf_uris[0]}\")\n",
    "    ds = tiledbvcf.Dataset(array_uri, mode = \"w\", cfg = cfg)\n",
    "    ds.ingest_samples(sample_uris = bcf_uris, threads = 2, memory_budget_mb = 512)\n",
    "    return bcf_uris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-passenger",
   "metadata": {},
   "source": [
    "This function requires 3 inputs: a URI for the array, a list of URIs for the BCF files, and a dictionary containing the necessary AWS parameters. It returns the same list of BCF URI's simply to pass them to a second function that collects the same output from all of the parallel UDFs to serve as a root dependency task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "breeding-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_samples(file_list):\n",
    "    out = []\n",
    "    [out.extend(i) for i in file_list]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-interaction",
   "metadata": {},
   "source": [
    "Next, we'll split our list of BCF URIs into groups, each of which represents a single UDF task. It is recommended that the group sizes are equal to the ingestion `sample_batch_size` paramter to avoid creating more fragments than necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "batched_uris = [bcf_uris[i * n:(i + 1) * n] for i in range((len(bcf_uris) + n - 1) // n )]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-sixth",
   "metadata": {},
   "source": [
    "Just as we did above, we're going to create a new dataset to ingest our BCF files into, the only difference being we need to pass along our AWS credenetials to grant the nodes access. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-honolulu",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_uri2 = f\"s3://{aws_bucket}/{array_prefix}_udf\"\n",
    "\n",
    "cfg = tiledbvcf.ReadConfig(tiledb_config = {\n",
    "    \"vfs.s3.aws_access_key_id\": os.getenv(\"TILEDB_VFS_S3_AWS_ACCESS_KEY_ID\"),\n",
    "    \"vfs.s3.aws_secret_access_key\": os.getenv(\"TILEDB_VFS_S3_SECRET_ACCESS_KEY\")\n",
    "})\n",
    "\n",
    "ds = tiledbvcf.Dataset(uri = array_uri2, mode = \"w\", stats = True, verbose = True, cfg = cfg)\n",
    "ds.create_dataset(extra_attrs = [\"info_GT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-elite",
   "metadata": {},
   "source": [
    "Finally, we'll create delayed versions of this functions populated with the necessary argument values and then execute the ingestion by calling `compute()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "delayed_writes = [Delayed(ingest_bcf_files)(array_uri, b, cfg) for b in batched_uris]\n",
    "ingested_samples = Delayed(return_samples, name = \"Combine\")(delayed_writes)\n",
    "\n",
    "ingested_samples.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-enclosure",
   "metadata": {},
   "source": [
    "### UDF-based Queries\n",
    "\n",
    "We can follow a very similar process to parallelize queries. First, we'll need to create a callable function that performs our query for a specific sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_vcf_dataset(sample):\n",
    "    ds = tiledbvcf.Dataset(array_uri, mode = \"r\")\n",
    "    return ds.read(attrs, samples = [sample], bed_file = bed_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-validity",
   "metadata": {},
   "source": [
    "We then need to create a *delayed* instance of this `query_vcf_dataset()` function for each sample in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-tamil",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dfs = [Delayed(query_vcf_dataset)(sample = s) for s in ds.samples()[0:99]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-venezuela",
   "metadata": {},
   "source": [
    "After execution, sample_dfs will comprise a list of dataframes, each containing the sample-specific results. Let's add one more UDF that will combine the individual dataframes into a single result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-stewart",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_results(df_list):\n",
    "    return pd.concat(df_list)\n",
    "\n",
    "combined_df = Delayed(combine_results)(sample_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-waste",
   "metadata": {},
   "source": [
    "\n",
    "At this point, we have constructed a UDF task graph containing 10 query tasks that will be performed in parallel and a merge step to combine the results. Let's visualize the task graph to verify the number of tasks and inter-dependencies look correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-violation",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.visualize(force_plotly = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-healing",
   "metadata": {},
   "source": [
    "Finally, let's execute our serverless distributed queries. If you're running this tutorial interactively, keep an eye on the diagram above. The nodes are color coded to indicate the current status of each task and update in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-farmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
