{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "TileDB-VCF offers several interfaces to create and/or query TileDB arrays containing genomic variant data. Here we'll utilize the command-line interface (CLI) and walk through the process of loading raw VCF data into TileDB and then performing a few different types of exports.\n",
    "\n",
    "***Hint: By the way, you can launch an interactive version of this tutoral using [TileDB Cloud Notebooks](https://console.tiledb.com/notebooks). Simply start a session with the Genomics & Geospatial image and use the integrated file browser to open the notebook: `examples/genomics/01-vcf-cli.ipynb`.***\n",
    "\n",
    "\n",
    "# Create a Dataset\n",
    "\n",
    "The process of creating a new VCF dataset involves 3 phases: `create`, `register`, and `store`. Detailed information about each phase is provided in the [ingestion algorithm](https://docs.tiledb.com/genomics/advanced/ingestion-algorithm) section.\n",
    "\n",
    "\n",
    "We'll start with a small example using 3 synthetic VCF files available in `data/vcfs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index files are required for ingestion. If your VCF/BCF files have not been indexed you can use [`bcftools`](https://samtools.github.io/bcftools/bcftools.html) to do so:\n",
    "\n",
    "```shell\n",
    "for f in data/vcfs/*.vcf.gz; do bcftools index -c $f; done\n",
    "```\n",
    "\n",
    "The first step is to **create** an empty dataset. Let's save the dataset in a new directory called `small_dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiledbvcf create --uri small_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to **register** the samples that will be ingested using either a text file that contains the location of each VCF or by directly passing a list of VCF files to the command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiledbvcf register --uri small_dataset data/vcfs/G*.vcf.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can always determine which samples have been registered with a dataset using the `list` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tiledbvcf list --uri small_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to actually ingest the VCF files and **store** their content within `small_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiledbvcf store --uri small_dataset data/vcfs/G*.vcf.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Let's verify evertyhing went okay using the `stat` command to provide high-level statistics about our dataset including the number of samples it contains and the variant attributes it includes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiledbvcf stat --uri small_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you have succesfully created and populated a TileDB VCF dataset. By default *all* metadata recorded in the VCF data lines are ingested but you can override this behavior with the `--attributes` flag, which allows you to specify the subset of fields you want to include.\n",
    "\n",
    "\n",
    "\n",
    "# Incremental Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One key advantage to using TileDB as the data store for your genomic variant data is the ability to efficiently add new samples as they become available. Furthermore, TileDB's native cloud features make it possible to ingest samples directly from remote locations. Here, we'll register and ingest the following additional samples, which are located on [AWS S3](https://aws.amazon.com/s3/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat data/s3-bcf-samples.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: Samples in this second batch are stored as `BCF` files which are also supported by TileDB-VCF.*\n",
    "\n",
    "This process is identical to the steps perfomed above, the only changes needed to our code involve setting `--scratch-mb` to allocate some temporary space for downloading the files and providing the URLs for the remote files. In this case, we'll simply pass the `s3-bcf-samples.txt` file, which includes a list of the BCF files we want to register and ingest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiledbvcf register \\\n",
    "  --uri small_dataset \\\n",
    "  --scratch-mb 1 \\\n",
    "  --samples-file data/s3-bcf-samples.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: When ingesting samples from S3, you must configure enough scratch space to hold at least 20 samples. In general, you need 2 &times; the sample dimension's `tile_extent` (which by default is 10). You can read more about the data model [here](https://docs.tiledb.com/genomics/advanced/data-mode)).*\n",
    "\n",
    "You can add the `--verbose` flag to print out more information during the `store` phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiledbvcf store \\\n",
    "  --uri small_dataset \\\n",
    "  --scratch-mb 10 \\\n",
    "  --samples-file data/s3-bcf-samples.txt \\\n",
    "  --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's run the `stat` command to verify our dataset now includes 10 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiledbvcf stat --uri small_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you've updated an existing dataset by adding new *BCF files* located on a *remote* server. Because TileDB is designed to be updatable, this process happens efficiently and without ever touching the previously ingested dataâ€”avoiding computationally expensive operations like regenerating combined VCF files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying and Exporting\n",
    "\n",
    "Now that you've created a TileDB-VCF dataset, you can begin to query the variant data for the 10 ingested samples. As mentioned earlier, several APIs are available for accessing the data. See the [Python usage section](https://docs.tiledb.com/genomics/usage/python) for examples using the `tiledbvcf` Python module. Here, we'll continue working with the CLI, focusing now on the `export` command, which can produce 3 different types of outputs for any given query:\n",
    "\n",
    "1. VCF (or BCF), producing one `.vcf` output file per exported sample\n",
    "2. Tabular, producing a single tab-separated value (TSV) text file containing all intersecting records across the exported samples\n",
    "3. Count-only, outputs a count of the total number of intersecting records (no output file is produced)\n",
    "\n",
    "The following examples provide a high-level overview of the CLI's export functionality; see `tiledbvcf export --help` or the [TileDB-VCF website](https://docs.tiledb.com/genomics/apis/cli) for comprehensive documentation.\n",
    "\n",
    "## Export VCF/BCFs\n",
    "\n",
    "In this example we will export several genomic regions from `small_dataset` created above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p data/exported-subsets\n",
    "\n",
    "tiledbvcf export \\\n",
    "  --uri small_dataset \\\n",
    "  --regions 1:1-50000,2:1-50000,3:1-50000 \\\n",
    "  --sample-names G1,G2,G3 \\\n",
    "  --output-dir data/exported-subsets \\\n",
    "  --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produced the three `bcf` files shown below, each of which contains the records intersecting the specified regions for the corresponding sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree data/exported-subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `bcftools` to examine any of the exported `bcf` files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcftools view --no-header data/exported-subsets/G1.bcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note: If a sample does not contain any intersecting records an output file is still created but it will include 0 records.***\n",
    "\n",
    "In order to export *all* records, simply omit the `--regions` argument. For example, the following recovers the original BCFs for samples `G4` and `G5`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p data/exported-full\n",
    "\n",
    "tiledbvcf export \\\n",
    "  --uri small_dataset \\\n",
    "  --sample-names G4,G5 \\\n",
    "  --output-dir data/exported-full \\\n",
    "  --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will output two BCFs for the corresponding samples that include the complete set of information from the original files. Note, while these exports are *lossless* in terms of the actual data stored, they may not be *identical* to the original files. Foe example, exported BCF/VCFs will always include an `END` position field even if one was not present in the ingested files. Furthermore, the ordering of fields within the `INFO` and `FORMAT` columns may also differ. \n",
    "\n",
    "## Export to TSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll look at exporting data from `small_dataset` in *tabular* format, which makes it convenient to load into other tools for downstream analyses. The command arguments are largely the same as for exporting to VCF/BCF, you just need to change the output type to `t` (short for TSV) and specify which VCF fields should be included as columns in the output table. The standard field names are:\n",
    "\n",
    "- `SAMPLE` (always included)\n",
    "- `ID`\n",
    "- `REF`\n",
    "- `ALT`\n",
    "- `QUAL`\n",
    "- `CHR`\n",
    "- `POS`\n",
    "- `FILTER`\n",
    "\n",
    "Fields within the `FORMAT` and `INFO` columns are accessed using special prefixes, `S:` and `I:`, respectively. This notation is used below to include genotype information from the `FORMAT:GT` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tiledbvcf export \\\n",
    "    --uri small_dataset \\\n",
    "    -Ot --tsv-fields CHR,POS,I:END,REF,S:GT \\\n",
    "    -s G1,G2,G3,G4 \\\n",
    "    --regions 1:1-50000 \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are output in a long form table where each row represents a single variant that overlapped one of the specified query regions for a particular sample. \n",
    "\n",
    "There's one more special prefix worth pointing out: `Q:` (for *query*), which allows you to include columns containing the start (`Q:POS`) and end (`Q:END`) positions for the query region in which the variant is located. Here we perform the same query across a few more regions, add the query regions columns, and save the output to `data/exported-regions.tsv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiledbvcf export \\\n",
    "    --uri small_dataset \\\n",
    "    -Ot --tsv-fields Q:POS,Q:END,CHR,POS,I:END,REF,S:GT \\\n",
    "    -s G1,G2,G3,G4 \\\n",
    "    --regions 1:1-50000,2:60000-100000,3:110000-160000 \\\n",
    "    --output-path data/exported-regions.tsv\n",
    "\n",
    "cat data/exported-regions.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first row shows that sample `G3` contains a variant stretching from 11,758â€“92,302 on chromosome 1 overlaps the first query region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo \"Updated: $(date)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiledbvcf --version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
