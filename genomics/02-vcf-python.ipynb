{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The `tiledbvcf` Python module allows you to create, update, and query TileDB-VCF datasets. It was designed to facilitate convenient analysis of genomic variant data through integration with Pandas. It also provides convenient interfaces for batch-processing queries and natively supports remote remote data stores like S3. \n",
    "\n",
    "*Hint: By the way, you can launch an interactive version of this tutoral using TileDB Cloud Notebooks. Simply start a session with the Genomics & Geospatial image and use the integrated file browser to open the notebook: examples/genomics/02-vcf-python.ipynb.*\n",
    "\n",
    "\n",
    "# Setup\n",
    "\n",
    "This tutorial requires the following modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiledbvcf\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing VCF files in TileDB\n",
    "\n",
    "Similar to TileDB-VCF's command-line interface (CLI), `tiledbvcf` supports ingesting VCF (or BCF) files into TileDB, either when creating a new dataset *or* updating an existing datset with additional samples. See the [CLI tutorial](https://docs.tiledb.com/genomics/usage/cli) for a more detailed description of the ingestion process. Here, we'll only focus on the mechanics of ingestion from Python.\n",
    "\n",
    "## Local VCFs\n",
    "\n",
    "The `data/vcf` directory contains sample-level `vcf` files for the first 3 individuals from a synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_vcfs = glob.glob(\"data/vcfs/*vcf.gz\")\n",
    "local_vcfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can ingest these files into TileDB an empty **TileDB VCF dataset** must first be created and opened in *write* mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ds = tiledbvcf.Dataset('data/small_dataset2', mode = \"w\")\n",
    "small_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a new `Dataset` object, into which the VCF files can be ingested and stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ds.ingest_samples(local_vcfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now re-open the dataset in *read* mode and begin to query it. For example, use `count()` to return the total number of ranges, across all samples contained within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ds = tiledbvcf.Dataset('data/small_dataset2', mode = \"r\")\n",
    "small_ds.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remote VCFs\n",
    "\n",
    "As of v0.5, `tiledbvcf` supports ingesting VCF files directly from remote locations, like AWS S3. The text file `data/s3-bcf-samples.txt` contains a list of S3 URIs pointing to 7 BCF files from the same cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/s3-bcf-samples.txt\") as f:\n",
    "    sample_uris = [l.rstrip(\"\\n\") for l in f.readlines()]\n",
    "\n",
    "sample_uris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add them into your existing dataset by re-opening it in *write* mode and providing the file URIs. It's also necessary to allocate scratch space so the files can be downloaded to a temporary location prior to ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ds = tiledbvcf.Dataset('data/small_dataset2', mode = \"w\")\n",
    "\n",
    "small_ds.ingest_samples(\n",
    "    sample_uris, \n",
    "    scratch_space_path = tempfile.gettempdir(), \n",
    "    scratch_space_size=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TileDB-VCF dataset located at `data/small_dataset2` now includes records for 660 variants across 10 samples. The next section provides examples demonstrating how to query this dataset.\n",
    "\n",
    "*Hint: To facilitate large ingestions, TileDB-VCF also integrates with [AWS Batch](https://aws.amazon.com/batch/) and provides a [helper script](https://github.com/TileDB-Inc/TileDB-VCF/blob/master/apis/aws-batch/batch-ingest.py) to use this service for parallelizing ingestion across a user-defined number of batches.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying Variant Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tiledbvcf.read()` function is your primary Python interface to the variant data stored in a TileDB-VCF dataset. It allows you to perform highly targeted queries and extract data for any subset of variants, samples, or attributes. To facilitate convenient downstream analysis, `tiledbvcf.read()` always returns results as a *pandas* `Data.Frame`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a small example focused on a few genes of interest. The genes are stored in a `dict` that indexes each gene's genomic range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = {\n",
    "    \"ZNF595\": \"4:53227-196092\",\n",
    "    \"DOCK8\":  \"9:214865-465259\",\n",
    "    \"DIP2C\":  \"10:320130-735608\",\n",
    "    \"RPH3AL\": \"17:62180-202633\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, make sure you have opened the dataset in *read* mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ds = tiledbvcf.Dataset('data/small_dataset2', mode = \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we query our dataset for any variants that overlap *ZNF595* and specify which attributes to include in the results `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ds.read(\n",
    "  attrs = ['sample_name', 'pos_start'], \n",
    "  samples = [\"G1\", \"G2\"],\n",
    "  regions = [genes.get(\"ZNF595\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `attrs` argument accepts the following values:\n",
    "* `sample_name`\n",
    "* `id` (variant identifier)\n",
    "* `pos_start`\n",
    "* `pos_end`\n",
    "* `alleles`\n",
    "* `filters`\n",
    "\n",
    "Additionally, any individual `FORMAT` or `INFO` fields that were ingested into the dataset can be included by adding a `fmt_` or `info_` prefix, respectively. The TileDB-VCF schema uses these prefixes to avoid ambiguity in cases where an attribute like *DP* could come from the *FORMAT* or *INFO* field.\n",
    "\n",
    "*Hint: See the [API reference](https://docs.tiledb.com/genomics/apis/python#read) for a complete list of queryable attributes.*\n",
    "\n",
    "Let's expand this query to include a list of ranges so that all gene regions of interest are scanned for hits simultaneously. Additionally, search across *all* samples in the dataset, and include each variant's alleles and each sample's genotypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\"G\" + str(i) for i in range(1,11)]\n",
    "\n",
    "df = small_ds.read(\n",
    "  attrs = [\"sample_name\", 'contig', 'pos_start', 'pos_end', 'alleles', 'fmt_GT'], \n",
    "  samples = samples,\n",
    "  regions = list(genes.values())\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output format from `read()` is always a `DataFrame` in *\"long\"* format, in which each row represents an individual variant indexed by a specific sample and genomic position. However, you can easily reformat into a *\"wide\"* variant &times; sample genotype matrix using the `pivot()` method provided by *pandas*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide = df.copy()\n",
    "df_wide = df_wide.set_index([\"contig\", \"pos_start\"])\n",
    "df_wide[[\"sample_name\", \"fmt_GT\"]].pivot(columns = \"sample_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you've created and queried a small example dataset. Next you will work with a more realistically sized dataset and utilize `tiledfvcf`'s features for handling large queries, which can enable incremental/out-of-core processing in the event that the requested data is too large to fit in main memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Large Queries\n",
    "\n",
    "Unlike the CLI, which exports directly to disk, results for queries performed using Python are read into memory. Therefore, when querying even moderately sized genomic datasets, the amount of available memory must be taken into consideration. \n",
    "\n",
    "Here, we will demonstrate several of `tilebvcf`'s features for overcoming memory limitations when querying large datasets. To do so, the following examples will utilize a new example TileDB-VCF dataset that contains 20 samples with over 20 million variants each. This dataset is publicly available on S3 and TileDB Cloud, and can be access with `tiledbvcf` by simply providing the appropriate URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiledbvcf\n",
    "uri = \"data/vcf-samples-20\"\n",
    "\n",
    "# uri = \"tiledb://vcf-samples-20\"\n",
    "# uri = \"s3://tiledb-inc-demo-data/tiledb-arrays/2.0/vcf-samples-20\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Budget\n",
    "\n",
    "One strategy for accommodating large queries is to simply increase the amount of memory available to `tiledbvcf`. By default `tiledbvcf` allocates 2Gb of memory for queries. However, this value can be adjusted using the `memory_budget_mb` parameter. For the purposes of this tutorial the budget will be *decreased* to demonstrate how `tiledbvcf` is able to perform genome-scale queries even in a memory constrained environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = tiledbvcf.ReadConfig(memory_budget_mb=256)\n",
    "ds = tiledbvcf.Dataset(uri, mode = \"r\", cfg = cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read more about how TileDB-VCF allocates memory [here](https://docs.tiledb.com/genomics/advanced/read-algorithm#buffer-allocation).\n",
    "\n",
    "## Batched Reads\n",
    "\n",
    "For queries that encompass many genomic regions you can simply provide an external `bed` file. In this example, you will query for any variants located in the promoter region of a known gene located on chromosomes 1–4. \n",
    "\n",
    "After performing a query, you can use `read_completed()` to verify whether or not *all* results were succesfully returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs = [\"sample_name\", \"contig\", \"pos_start\", \"fmt_GT\"]\n",
    "df = ds.read(attrs, bed_file = \"data/gene-promoters-hg38.bed\")\n",
    "ds.read_completed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, it returned `False`, indicating the requested data was too large to fit in memory so `tiledbvcf` retrieved as many records as possible in this first batch. The reamining records can be retrieved using `continue_read()`. Here, we've setup our code to accomodate the possibility that the full set of results are split across multiple batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"The dataframe contains\")\n",
    "\n",
    "while not ds.read_completed():\n",
    "    print (f\"\\t...{df.shape[0]} rows\")\n",
    "    df = df.append(ds.continue_read())\n",
    "\n",
    "print (f\"\\t...{df.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration\n",
    "\n",
    "A Python generator version of read function is also provided. This pattern provides a powerful interface for batch processing variant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tiledbvcf.Dataset(uri, mode = \"r\", cfg = cfg)\n",
    "\n",
    "attrs.append(\"query_bed_start\")\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for batch in ds.read_iter(attrs, bed_file = \"data/gene-promoters-hg38.bed\"):\n",
    "    df = df.append(batch, ignore_index = True)\n",
    "    \n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
