{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TileDB VCF Example for 1000 Genomes Project\n",
    "\n",
    "TileDB Cloud's Public Data Explorer provides access to a complete version of the [1000 Genomes Project](https://www.internationalgenome.org/) Phase 3 data stored as a TileDB-VCF Dataset. This tutorial provides a walkthrough of the steps we followed to create this resource using [TileDB-VCF](https://github.com/TileDB-Inc/TileDB-VCF.git)'s Python package. \n",
    "\n",
    "You will learn:\n",
    "* how TileDB-VCF can easily scale to handle datasets of any size through its ability to simultaneously read from and write to remote data-object stores, like AWS S3\n",
    "* query the dataset via Python and access the results as Pandas `Dataframes`\n",
    "* pre-process combined project VCF files (pVCF) into single sample VCF files appropriate for TileDB-VCF\n",
    "\n",
    "Please see the [official documentation](https://docs.tiledb.com/solutions/integrations/population-genomics) for more comprehensive usage details, API references, as well as instructions for installing TileDB-VCF.\n",
    "\n",
    "## Preprocessing the Raw 1KG pVCF File\n",
    "\n",
    "Unlike the more modern [high-coverage version](https://www.internationalgenome.org/announcements/3202-samples-at-high-coverage-from-NYGC/) of the 1000 Genomes (1KG) data, which provides raw single-sample gVCF files, the low-coverage Phase 3 data only provides chromosome-specific pVCF files that combine variant calls for all 2,504 samples. In order to ingest this data into a TileDB-VCF dataset we must convert the densified pVCF files into sparse single-sample VCF files.\n",
    "\n",
    "*Note: We're only working with data from chromosome 1 for the purposes of this tutorial. However, the process of working with whole genome data is the same.*\n",
    "\n",
    "The original VCF file was downloaded from the AWS Open Data Registry: <a href=\"https://registry.opendata.aws/1000-genomes\" class=\"uri\">https://registry.opendata.aws/1000-genomes</a>.\n",
    "\n",
    "```sh\n",
    "aws s3 sync \\\n",
    "    --exclude \"*\" --include \"ALL.chr1.*\" \\\n",
    "    s3://1000genomes/release/20130502/ \\\n",
    "    data/1000genomes/\n",
    "```\n",
    "\n",
    "First, we use `bcftools` to split the pVCF file back into single-sample VCF files\n",
    "\n",
    "```sh\n",
    "bcftools +split \\\n",
    "    -Ob \\\n",
    "    -o data/split-bcfs \\\n",
    "    data/1000genomes/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\n",
    "```\n",
    "\n",
    "Next we’ll filter the split VCF files to include only records with a non-reference allele and remove `INFO` attributes that are either static (e.g., `NS`) or cohort-specific and recoverable (e.g., `AF`). We’ll save the final pre-processed files in *BCF* format, which is the binary representation of the VCF format.\n",
    "\n",
    "```sh\n",
    "rm_tags=INFO/AF,INFO/NS,INFO/EAS_AF,INFO/AMR_AF,INFO/AFR_AF,INFO/EUR_AF,INFO/SAS_AF\n",
    "\n",
    "ls data/split-bcfs/*.bcf | parallel -j16 \\\n",
    "    \"bcftools view --min-ac 1 -Ou -s {/.} {} | bcftools annotate -Ob --remove $rm_tags -o data/filtered-bcfs/{/}\"\n",
    "```\n",
    "\n",
    "The resulting filtered BCF files are a close approximation of the raw single sample VCF files typically stored for large population genomics projects. The last step is to index the newly created BCF files.\n",
    "\n",
    "```sh\n",
    "ls data/filtered-bcfs/*.bcf | parallel -j16 \"bcftools index {}\"\n",
    "```\n",
    "\n",
    "For convenience, we have stored the pre-processed genome-wide single-sample VCF files on S3: \n",
    "\n",
    "```sh\n",
    "aws s3 cp --recursive data/filtered-bcfs/ s3://genomic-datasets/notebooks/1kgp3/filtered-bcfs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Data in TileDB VCF\n",
    "\n",
    "We'll switch to Python for the rest of this tutorial and use the `tiledbvcf` package to ingest and query the pre-processed 1KG variant data. The only other package we'll need is `boto3` to provide a list of the VCF files S3 URIs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'0.7.2.dev14'"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "import boto3\n",
    "import tiledbvcf\n",
    "# from envbash import load_envbash\n",
    "\n",
    "tiledbvcf.version"
   ]
  },
  {
   "source": [
    "We'll also define define a few variables that we'll refer to throughout the tutorial."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_bucket=\"genomic-datasets\"\n",
    "array_prefix=\"1kg/1kgdbv5\"\n",
    "array_uri = f\"s3://{aws_bucket}/{array_prefix}\"\n",
    "\n",
    "bcfs_prefix=\"1kg/1kgdbv5_supp\"\n",
    "bcf_uris = []\n",
    "\n",
    "bedfile = \"s3://genomic-datasets/notebooks/1kgp3/hg37_chr1_covidHgiGwasR4PvalC2_plog3.bed.gz\""
   ]
  },
  {
   "source": [
    "The following was run on a `m5.4xlarge` system with a 300GB EBS volume to handle the large number of VCF files. Note that TileDB is *highly-tunable* and while the defaults were chosen to provide a good balance between ingestion speed, read performance, and dataset size, they can be be tweaked to better suit a specific use case.\n",
    "\n",
    "### Create the dataset\n",
    "\n",
    "You can create a TileDB VCF dataset anywhere that TileDB supports, this could be a local filesystem, S3, Azure, Google Cloud Storage, HDFS, and more. For this example we’ll create it on S3, the most common use case.\n",
    "\n",
    "We're opening the dataset in *write* mode, with `verbose = True` to receive information about the progress of the ingestion, and `stats = True` to get some insight into TileDB's performance after ingestion completes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'s3://genomic-datasets/1kg/1kgdbv5_test'"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "array_uri = array_uri + \"_test\"\n",
    "array_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tiledbvcf.dataset.Dataset at 0x7fb72eafd290>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "ds = tiledbvcf.Dataset(array_uri, stats = True, verbose = True, mode = \"w\")\n",
    "\n",
    "ds.create_dataset(extra_attrs = [\"fmt_GT\"])"
   ]
  },
  {
   "source": [
    "In order to ingest the BCF files from S3 without downloading them we need a list of URIs that point to each files' location.\n",
    "\n",
    "\n",
    "Next, we'll filter the list to remove the index files and create properly formatted S3 URIs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing the data\n",
    "\n",
    "Storing VCF data in TileDB-VCF you simply need a list of the VCF/BCF file locations. In this case, we'll provide a list of S3 URIS pointing to the BCF files, which will allow us to ingest them directly from their remote location.\n",
    "\n",
    "We're using `boto3` to list the all the files in our S3 bucket containing the BCF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "List URIs for files in an S3 bucket\n",
    ":param bucket (string) Bucket name to list.\n",
    ":param prefix (string) Limits the response to keys that begin with the specified prefix\n",
    ":param suffix (string, optional) Limits the response to files with the specified extension\n",
    "\"\"\"\n",
    "def aws_s3_ls(bucket, prefix, suffix = None):\n",
    "    s3 = boto3.client('s3')\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "    response_files = response[\"Contents\"]\n",
    "    list_incomplete = response[\"IsTruncated\"]\n",
    "\n",
    "    while list_incomplete:\n",
    "        response = s3.list_objects_v2(\n",
    "            Bucket=bucket, \n",
    "            Prefix=prefix,\n",
    "            ContinuationToken = response[\"NextContinuationToken\"]\n",
    "        )\n",
    "        response_files.extend(response[\"Contents\"])\n",
    "        list_incomplete = response[\"IsTruncated\"]\n",
    "\n",
    "    output = [f\"s3://{bucket}/{file['Key']}\" for file in response_files]\n",
    "\n",
    "    if suffix is not None:\n",
    "        output = [file for file in output if file.endswith(suffix)]\n",
    "\n",
    "    return output\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcf_uris = aws_s3_ls(aws_bucket, bcfs_prefix, suffix = \"bcf\")"
   ]
  },
  {
   "source": [
    "Finally, we'll run the following command to ingest the pre-processed BCF files into a new TileDB-VCF dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.ingest_samples(\n",
    "    sample_uris = bcf_uris,\n",
    "    threads = 12,\n",
    "    memory_budget_mb = 2048 * 2,\n",
    "    # record_limit = 100000,\n",
    "    sample_batch_size = 20,\n",
    "    scratch_space_path = \"/mnt/data/tmp\",\n",
    "    scratch_space_size = 1024\n",
    ")"
   ]
  },
  {
   "source": [
    "We can see from the verbose output provided the following summary printed at the end:\n",
    "\n",
    "```\n",
    "All finalize tasks successfully completed. Waited for 5.87609 sec.\n",
    "Done. Ingested 10,771,609,147 records (+ 38,528,651 anchors) from 2,504 samples in 24,546.9 seconds.\n",
    "```\n",
    "\n",
    "This indicates we’ve ingested over 10 billion records into the TileDB-VCF dataset in just under 7 hours. With an `m5.4xlarge` instance costing \\$0.768 an hour, the cost of ingestion was just over. \\$5.00 USD.\n",
    "\n",
    "The final array is 6Gb, or just under half the size of the individual compressed BCF files."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n==== WRITE ====\n\n- Number of write queries: 70434\n\n- Number of attributes written: 633906\n  * Number of fixed-sized attributes written: 211302\n  * Number of var-sized attributes written: 422604\n- Number of dimensions written: 211302\n  * Number of fixed-sized dimensions written: 70434\n  * Number of var-sized dimensions written: 140868\n\n- Number of bytes written: 75362471867 bytes (70.1868 GB) \n- Number of write operations: 21863256\n- Number of bytes filtered: 2278398374656 bytes (2121.92 GB) \n- Filtering deflation factor: 30.2325x\n\n- Total metadata written: 89454680 bytes (0.0833112 GB) \n  * Array schema: 813 bytes (7.57165e-07 GB) \n  * Fragment metadata footer: 1605894 bytes (0.00149561 GB) \n  * R-tree: 7523989 bytes (0.00700726 GB) \n  * Fixed-sized tile offsets: 38844555 bytes (0.0361768 GB) \n  * Var-sized tile offsets: 28188665 bytes (0.0262527 GB) \n  * Var-sized tile sizes: 13290764 bytes (0.012378 GB) \n\n- Time to write array metadata: 0.156775 secs\n  * Array metadata size: 110 bytes (1.02445e-07 GB) \n\n- Number of logical cells written: 10810137798\n- Number of logical tiles written: 1082318\n  * Number of fixed-sized physical tiles written: 304927944048\n  * Number of var-sized physical tiles written: 1219711776192\n\n- Write time: 8278.06 secs\n  * Time to split the coordinates buffer: 0.0454959 secs\n  * Time to check out-of-bounds coordinates: 68.1267 secs\n  * Time to check coordinate duplicates: 0.0474146 secs\n  * Time to check global order: 159.101 secs\n  * Time to prepare tiles: 2707.89 secs\n  * Time to compute coordinate metadata (e.g., MBRs): 294.138 secs\n  * Time to filter tiles: 2885.64 secs\n  * Time to write tiles: 2193.43 secs\n  * Time to write fragment metadata: 2039.88 secs\n\n- Time to finalize write query: 18168.3 secs\n\n\n"
     ]
    }
   ],
   "source": [
    "print(ds.tiledb_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following ingestion, it may help performance to consolidate the metadata fragments, which is currently only possible using the CLI.\n",
    "\n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tiledbvcf utils consolidate fragment_meta -u s3://genomic-datasets/notebooks/1kgp3/1kg-array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading, Analysis and Exporting\n",
    "\n",
    "In this section we will walk through accessing the TileDB-VCF dataset and also exporting back to VCF and TSV. First, we must reopen the dataset in *read* mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'tiledbvcf' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-55d1791ff54e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtiledbvcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tiledbvcf' is not defined"
     ]
    }
   ],
   "source": [
    "ds = tiledbvcf.Dataset(array_uri, stats = True, verbose = True, mode = \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading into Pandas Dataframe\n",
    "\n",
    "Pandas is one of the most popular data science tools in python. TileDB VCF’s python API produces results directly into a pandas dataframe. This makes its easy to analyze the data and leverage any of pandas’ builtin algorithms.\n",
    "\n",
    "Let’s run a typical query on the 1kg TileDB-VCF dataset we created above. We’ll retrieve all variants that overlap the gene *MTOR* on chr 1 for sample HG00096, along with a few attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   sample_name contig  pos_start   pos_end alleles  fmt_GT\n",
       "0      HG00096      1   43337897  43337897  [A, G]  [1, 0]\n",
       "1      HG00096      1   43339092  43339092  [C, T]  [1, 1]\n",
       "2      HG00096      1   43339203  43339203  [G, A]  [1, 1]\n",
       "3      HG00096      1   43340776  43340776  [T, C]  [1, 1]\n",
       "4      HG00096      1   43340779  43340779  [A, G]  [1, 1]\n",
       "5      HG00096      1   43341662  43341662  [A, G]  [1, 1]\n",
       "6      HG00096      1   43342021  43342021  [G, A]  [1, 0]\n",
       "7      HG00096      1   43343390  43343390  [A, G]  [1, 1]\n",
       "8      HG00096      1   43344193  43344193  [G, A]  [1, 1]\n",
       "9      HG00096      1   43344632  43344632  [T, A]  [1, 1]\n",
       "10     HG00096      1   43346551  43346551  [A, G]  [1, 1]\n",
       "11     HG00096      1   43347556  43347556  [C, G]  [1, 1]\n",
       "12     HG00096      1   43347925  43347925  [C, T]  [1, 1]\n",
       "13     HG00096      1   43348081  43348081  [G, T]  [1, 1]\n",
       "14     HG00096      1   43348184  43348184  [A, T]  [1, 1]\n",
       "15     HG00096      1   43350603  43350603  [C, T]  [1, 1]\n",
       "16     HG00096      1   43351984  43351984  [A, G]  [1, 1]\n",
       "17     HG00096      1   43352685  43352685  [A, G]  [1, 1]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_name</th>\n      <th>contig</th>\n      <th>pos_start</th>\n      <th>pos_end</th>\n      <th>alleles</th>\n      <th>fmt_GT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HG00096</td>\n      <td>1</td>\n      <td>43337897</td>\n      <td>43337897</td>\n      <td>[A, G]</td>\n      <td>[1, 0]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HG00096</td>\n      <td>1</td>\n      <td>43339092</td>\n      <td>43339092</td>\n      <td>[C, T]</td>\n      <td>[1, 1]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HG00096</td>\n      <td>1</td>\n      <td>43339203</td>\n      <td>43339203</td>\n      <td>[G, A]</td>\n      <td>[1, 1]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HG00096</td>\n      <td>1</td>\n      <td>43340776</td>\n      <td>43340776</td>\n      <td>[T, C]</td>\n      <td>[1, 1]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HG00096</td>\n      <td>1</td>\n      <td>43340779</td>\n      <td>43340779</td>\n      <td>[A, G]</td>\n      <td>[1, 1]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>HG00096</td>\n      <td>1</td>\n      <td>43341662</td>\n      <td>43341662</td>\n      <td>[A, G]</td>\n      <td>[1, 1]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>HG00096</td>\n      <td>1</td>\n      <td>43342021</td>\n      <td>43342021</td>\n      <td>[G, A]</td>\n      <td>[1, 0]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>HG00096</td>\n      <td>1</td>\n      <td>43343390</td>\n      <td>43343390</td>\n      <td>[A, G]</td>\n      <td>[1, 1]</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>HG00096</td>\n      <td>1</td>\n      <td>43344193</td>\n      <td>43344193</td>\n      <td>[G, A]</td>\n      <td>[1, 1]</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>HG00096</td>\n      <td>1</td>\n      <td>43344632</td>\n      <td>43344632</td>\n      <td>[T, A]</td>\n      <td>[1, 1]</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>HG00096</td>\n      <td>1</td>\n      <td>43346551</td>\n      <td>43346551</td>\n      <td>[A, G]</td>\n      <td>[1, 1]</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>HG00096</td>\n      <td>1</td>\n      <td>43347556</td>\n      <td>43347556</td>\n      <td>[C, G]</td>\n      <td>[1, 1]</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>HG00096</td>\n      <td>1</td>\n      <td>43347925</td>\n      <td>43347925</td>\n      <td>[C, T]</td>\n      <td>[1, 1]</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>HG00096</td>\n      <td>1</td>\n      <td>43348081</td>\n      <td>43348081</td>\n      <td>[G, T]</td>\n      <td>[1, 1]</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>HG00096</td>\n      <td>1</td>\n      <td>43348184</td>\n      <td>43348184</td>\n      <td>[A, T]</td>\n      <td>[1, 1]</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>HG00096</td>\n      <td>1</td>\n      <td>43350603</td>\n      <td>43350603</td>\n      <td>[C, T]</td>\n      <td>[1, 1]</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>HG00096</td>\n      <td>1</td>\n      <td>43351984</td>\n      <td>43351984</td>\n      <td>[A, G]</td>\n      <td>[1, 1]</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>HG00096</td>\n      <td>1</td>\n      <td>43352685</td>\n      <td>43352685</td>\n      <td>[A, G]</td>\n      <td>[1, 1]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "cfg = tiledbvcf.ReadConfig(memory_budget_mb=8192)\n",
    "ds = tiledbvcf.Dataset(array_uri, stats = True, cfg = cfg)\n",
    "\n",
    "ds.read(\n",
    "    attrs = [\"sample_name\", \"contig\", \"pos_start\", \"pos_end\", \"alleles\", \"fmt_GT\"], \n",
    "    regions = [\"1:43337848-43352772\"],\n",
    "    samples = [\"HG00096\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the `Dataset` object was created with `stats = True` you can print out a variety of useful information about the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==== READ ====\n\n- Number of read queries: 3\n- Number of attempts until results are found: 3\n\n- Number of attributes read: 5\n  * Number of fixed-sized attributes read: 2\n  * Number of var-sized attributes read: 3\n- Number of dimensions read: 5\n  * Number of fixed-sized dimensions read: 1\n  * Number of var-sized dimensions read: 4\n\n- Number of logical tiles overlapping the query: 10\n- Number of physical tiles read: 170\n  * Number of physical fixed-sized tiles read: 30\n  * Number of physical var-sized tiles read: 140\n- Number of cells read: 85008\n- Number of result cells: 2524\n- Percentage of useful cells read: 2.96913%\n\n- Number of bytes read: 40564739 bytes (0.0377789 GB) \n- Number of read operations: 115\n- Number of bytes unfiltered: 164678612 bytes (0.153369 GB) \n- Unfiltering inflation factor: 4.05965x\n\n- Time to compute estimated result size: 0.0217195 secs\n  * Time to compute tile overlap: 0.376641 secs\n    > Time to compute relevant fragments: 3.971e-05 secs\n    > Time to load relevant fragment R-trees: 0.37607 secs\n    > Time to compute relevant fragment tile overlap: 0.000513412 secs\n\n- Time to open array: 1.192 secs\n  * Time to load array schema: 0.0825531 secs\n  * Time to load consolidated fragment metadata: 0.0810633 secs\n  * Time to load fragment metadata: 0.115064 secs\n\n- Total metadata read: 119637165 bytes (0.111421 GB) \n  * Array schema: 813 bytes (7.57165e-07 GB) \n  * Consolidated fragment metadata: 1961072 bytes (0.00182639 GB) \n  * Fragment metadata: 802 bytes (7.46921e-07 GB) \n  * R-tree: 40181622 bytes (0.0374221 GB) \n  * Fixed-sized tile offsets: 36163320 bytes (0.0336797 GB) \n  * Var-sized tile offsets: 20664768 bytes (0.0192456 GB) \n  * Var-sized tile sizes: 20664768 bytes (0.0192456 GB) \n\n- Time to load array metadata: 0.057664 secs\n  * Array metadata size: 110 bytes (1.02445e-07 GB) \n\n- Time to initialize the read state: 0.355429 secs\n\n- Read time: 1.24297 secs\n  * Time to compute next partition: 0.144476 secs\n  * Time to compute result coordinates: 0.562097 secs\n    > Time to compute sparse result tiles: 2.2971e-05 secs\n    > Time to read coordinate tiles: 0.554943 secs\n    > Time to unfilter coordinate tiles: 0.00492478 secs\n    > Time to compute range result coordinates: 0.000745033 secs\n  * Time to compute sparse result cell slabs: 7.359e-06 secs\n  * Time to copy result attribute values: 0.53583 secs\n    > Time to read attribute tiles: 0.533097 secs\n    > Time to unfilter attribute tiles: 0.00198518 secs\n    > Time to copy fixed-sized attribute values: 0.000300206 secs\n    > Time to copy var-sized attribute values: 0.000300335 secs\n  * Time to copy result coordinates: 0.000535849 secs\n    > Time to copy fixed-sized coordinates: 0.000146444 secs\n    > Time to copy var-sized coordinates: 0.000342522 secs\n\n- Total read query time (array open + init state + read): 1.5984 secs\n\n\n\n\n"
     ]
    }
   ],
   "source": [
    "print(ds.tiledb_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For production-sized queries that encompass large portions of the genome it's more convenient to provide bed files with the query regions. Here, we'll use a bed file on s3 that contains 1,040 regions on chr1 that show at least a moderate association with with SARS-CoV-2 infection susceptibility (data obtained from the [COVID-19 Host Genetics Initiative](https://www.covid19hg.org/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     sample_name contig  pos_start    pos_end alleles  info_DP  fmt_GT\n",
       "0        HG00097      1    2265099    2265099  [C, T]  [16479]  [0, 1]\n",
       "1        HG00099      1    2265099    2265099  [C, T]  [16479]  [0, 1]\n",
       "2        HG00107      1    2265099    2265099  [C, T]  [16479]  [1, 0]\n",
       "3        HG00096      1    2337537    2337537  [C, G]  [11721]  [1, 1]\n",
       "4        HG00097      1    2337537    2337537  [C, G]  [11721]  [1, 1]\n",
       "...          ...    ...        ...        ...     ...      ...     ...\n",
       "3470     HG00102      1  247999680  247999680  [G, A]  [19889]  [1, 1]\n",
       "3471     HG00103      1  247999680  247999680  [G, A]  [19889]  [1, 1]\n",
       "3472     HG00105      1  247999680  247999680  [G, A]  [19889]  [1, 1]\n",
       "3473     HG00106      1  247999680  247999680  [G, A]  [19889]  [1, 1]\n",
       "3474     HG00107      1  247999680  247999680  [G, A]  [19889]  [1, 1]\n",
       "\n",
       "[3475 rows x 7 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_name</th>\n      <th>contig</th>\n      <th>pos_start</th>\n      <th>pos_end</th>\n      <th>alleles</th>\n      <th>info_DP</th>\n      <th>fmt_GT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HG00097</td>\n      <td>1</td>\n      <td>2265099</td>\n      <td>2265099</td>\n      <td>[C, T]</td>\n      <td>[16479]</td>\n      <td>[0, 1]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HG00099</td>\n      <td>1</td>\n      <td>2265099</td>\n      <td>2265099</td>\n      <td>[C, T]</td>\n      <td>[16479]</td>\n      <td>[0, 1]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HG00107</td>\n      <td>1</td>\n      <td>2265099</td>\n      <td>2265099</td>\n      <td>[C, T]</td>\n      <td>[16479]</td>\n      <td>[1, 0]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HG00096</td>\n      <td>1</td>\n      <td>2337537</td>\n      <td>2337537</td>\n      <td>[C, G]</td>\n      <td>[11721]</td>\n      <td>[1, 1]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HG00097</td>\n      <td>1</td>\n      <td>2337537</td>\n      <td>2337537</td>\n      <td>[C, G]</td>\n      <td>[11721]</td>\n      <td>[1, 1]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3470</th>\n      <td>HG00102</td>\n      <td>1</td>\n      <td>247999680</td>\n      <td>247999680</td>\n      <td>[G, A]</td>\n      <td>[19889]</td>\n      <td>[1, 1]</td>\n    </tr>\n    <tr>\n      <th>3471</th>\n      <td>HG00103</td>\n      <td>1</td>\n      <td>247999680</td>\n      <td>247999680</td>\n      <td>[G, A]</td>\n      <td>[19889]</td>\n      <td>[1, 1]</td>\n    </tr>\n    <tr>\n      <th>3472</th>\n      <td>HG00105</td>\n      <td>1</td>\n      <td>247999680</td>\n      <td>247999680</td>\n      <td>[G, A]</td>\n      <td>[19889]</td>\n      <td>[1, 1]</td>\n    </tr>\n    <tr>\n      <th>3473</th>\n      <td>HG00106</td>\n      <td>1</td>\n      <td>247999680</td>\n      <td>247999680</td>\n      <td>[G, A]</td>\n      <td>[19889]</td>\n      <td>[1, 1]</td>\n    </tr>\n    <tr>\n      <th>3474</th>\n      <td>HG00107</td>\n      <td>1</td>\n      <td>247999680</td>\n      <td>247999680</td>\n      <td>[G, A]</td>\n      <td>[19889]</td>\n      <td>[1, 1]</td>\n    </tr>\n  </tbody>\n</table>\n<p>3475 rows × 7 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "ds = tiledbvcf.Dataset(array_uri, stats = True, cfg = cfg)\n",
    "\n",
    "df = ds.read(\n",
    "    attrs = [\"sample_name\", \"contig\", \"pos_start\", \"pos_end\", \"alleles\", \"info_DP\", \"fmt_GT\"], \n",
    "    samples = ds.samples()[:10],\n",
    "    bed_file = bedfile\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This query completed in 1.9 secs for 10 samples.\n",
    "\n",
    "#### Filter Example\n",
    "\n",
    "Using the pandas dataframe returned from TileDB-VCF we can apply additional filters. For instance if we wanted to filter the above result on read depth (`info_DP`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.info_DP.apply(lambda x: x[0] > 5000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python API supports a variety of advanced uses, batching, partitioning, dask and more. We are happy to follow-up with additional details beyond these initial examples.\n",
    "\n",
    "### CLI Exporting\n",
    "\n",
    "In addition to the python API it is also possible to export the dataset back into VCF format. This can be helpful in interoperating with legacy tools.\n",
    "\n",
    "#### Exporting to VCF\n",
    "\n",
    "When exporting to VCF you can specify any number of samples, and each will be exported to its own file in vcf, compressed vcf or bcf format depdning on what you set for `--output-format`.\n",
    "\n",
    "For example to export the entire sample for `HG00096`, `HG00097`, and `HG00099` you can run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tiledbvcf export --uri s3://genomic-datasets/notebooks/1kgp3/1kg-array \\\n",
    "    --output-format v \\\n",
    "    --sample-names HG00096,HG00097,HG00099 \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces 3 files: `HG00096.vcf`, `HG00097.vcf`, and `HG00099.vcf`.\n",
    "\n",
    "##### Filtering Exports\n",
    "\n",
    "You can also combine the use of regions (bed file or list of regions passed to cli) to filter the export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tiledbvcf export --uri s3://genomic-datasets/notebooks/1kgp3/1kg3-array \\\n",
    "    --output-format v \\\n",
    "    --sample-names HG00096,HG00097,HG00099 \\\n",
    "    --regions-file s3://genomic-datasets/notebooks/1kgp3/hg37_chr1_covidHgiGwasR4PvalC2_plog3.bed.gz \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will also produce the 3 VCF files, like the previous export. However, these files are filtered for the same SARS-CoV-2 associated genomic regions specified in the bed file.\n",
    "\n",
    "\n",
    "#### Exporting to TSV\n",
    "\n",
    "For even more generic usecases you can export data to tab seperate files with the `--output-format t` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tiledbvcf export --uri s3://genomic-datasets/notebooks/1kgp3/1kg-array \\\n",
    "    --output-format t \\\n",
    "    --tsv-fields CHR,POS,I:END,REF,ALT,S:GT,Q:POS,Q:END \\\n",
    "    --sample-names HG00096,HG00097,HG00099 \\\n",
    "    --regions-file s3://genomic-datasets/notebooks/1kgp3/hg37_chr1_covidHgiGwasR4PvalC2_plog3.bed.gz \\\n",
    "    --verbose --output-path sars-cov-2-associated-regions.tsv"
   ]
  },
  {
   "source": [
    "# Scaling with Severless Computer\n",
    "\n",
    "## UDF-based Ingestion\n",
    "\n",
    "Now that we've seen the basic mechanics of how to create and query a TileDB-VCF dataset, we'll look at some options for solving the most common pain point in modern genomics analyses: scaling operations to process the massive cohorts produced by modern sequencing projects.\n",
    "\n",
    "Out of the box, TileDB supports fully parallel reads and writes so queries can be partitioned based on genomic regions, samples, or both, and distributed across multiple cores, with each core processing a different chunk of the data. TileDB-VCF provides a number of options to leverage these scaling features, including integrations with frameworks like Dask for parallel computing in Python, as well as Apache Spark.\n",
    "\n",
    "Here, we'll utilize TileDB Cloud's serverless compute service to easily convert our previous single-node ingestion into one that's automatically processed by as many nodes as we have sample batches. First, we'll need to import `tiledb.cloud`, which provides the `Delayed` module we'll use to convert our query function into a user-defined function (UDF) that can be serialized and shipped to serverless node for processing.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiledb.cloud.compute import Delayed"
   ]
  },
  {
   "source": [
    "Then we'll need to create a callable function that performs our ingestion for a given batch of samples."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_bcf_files(array_uri, bcf_uris, cfg):\n",
    "    print(f\"Ingesting {len(bcf_uris)} starting with {bcf_uris[0]}\")\n",
    "    ds = tiledbvcf.Dataset(array_uri, mode = \"w\", cfg = cfg)\n",
    "    ds.ingest_samples(sample_uris = bcf_uris)\n",
    "    return bcf_uris"
   ]
  },
  {
   "source": [
    "This function requires 3 inputs: a URI for the array, a list of URIs for the BCF files, and a dictionary containing the necessary AWS parameters. It returns the same list of BCF URI's simply to pass them to a second function that collects the same output from all of the parallel UDFs to serve as a root dependency task."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_samples(file_list):\n",
    "    out = []\n",
    "    [out.extend(i) for i in file_list]\n",
    "    return out"
   ]
  },
  {
   "source": [
    "Next, we'll split our list of BCF URIs into groups, each of which represents a single UDF task. It is recommended that the group sizes are equal to the ingestion `sample_batch_size` paramter to avoid creating more fragments than necessary. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "batched_uris = [bcf_uris[i * n:(i + 1) * n] for i in range((len(bcf_uris) + n - 1) // n )]"
   ]
  },
  {
   "source": [
    "Just as we did above, we're going to create a new dataset to ingest our BCF files into, the only difference being we need to pass along our AWS credenetials to grant the nodes access. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_prefix=\"1kg/1kgdbv5_udf\"\n",
    "array_uri = f\"s3://{aws_bucket}/{array_prefix}\"\n",
    "\n",
    "cfg = tiledbvcf.ReadConfig(tiledb_config = {\n",
    "    \"vfs.s3.aws_access_key_id\": os.getenv(\"TILEDB_VFS_S3_AWS_ACCESS_KEY_ID\"),\n",
    "    \"vfs.s3.aws_secret_access_key\": os.getenv(\"TILEDB_VFS_S3_SECRET_ACCESS_KEY\")\n",
    "})\n",
    "\n",
    "ds = tiledbvcf.Dataset(uri = array_uri, mode = \"w\", stats = True, verbose = True, cfg = cfg)\n",
    "ds.create_dataset(extra_attrs = [\"info_GT\"])"
   ]
  },
  {
   "source": [
    "Finally, we'll create delayed versions of this functions populated with the necessary argument values and then execute the ingestion by calling `compute()`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delayed_writes = [Delayed(ingest_bcf_files)(array_uri, b, cfg) for b in batched_uris]\n",
    "ingested_samples = Delayed(return_samples, name = \"Combine\")(delayed_writes)\n",
    "\n",
    "ingested_samples.compute()"
   ]
  },
  {
   "source": [
    "## UDF-based Queries\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We can follow a very similar process to parallelize queries. First, we'll need to create a callable function that performs our query for a specific sample:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_vcf_dataset(sample):\n",
    "    ds = tiledbvcf.Dataset(array_uri, mode = \"r\")\n",
    "    return ds.read(attrs, samples = [sample], bed_file = bed_file)"
   ]
  },
  {
   "source": [
    "We then need to create a *delayed* instance of this `query_vcf_dataset()` function for each sample in our dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dfs = [Delayed(query_vcf_dataset)(sample = s) for s in ds.samples()]"
   ]
  },
  {
   "source": [
    "After execution, sample_dfs will comprise a list of dataframes, each containing the sample-specific results. Let's add one more UDF that will combine the individual dataframes into a single result."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_results(df_list):\n",
    "    return pd.concat(df_list)\n",
    "\n",
    "combined_df = Delayed(combine_results)(sample_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'combined_df' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-3728ea468b11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcombined_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_plotly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'combined_df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "At this point, we have constructed a UDF task graph containing 10 query tasks that will be performed in parallel and a merge step to combine the results. Let's visualize the task graph to verify the number of tasks and inter-dependencies look correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.visualize(force_plotly = True)"
   ]
  },
  {
   "source": [
    "Finally, let's execute our serverless distributed queries. If you're running this tutorial interactively, keep an eye on the diagram above. The nodes are color coded to indicate the current status of each task and update in real time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "combined_df.compute()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "tiledbvcf-py",
   "display_name": "tiledbvcf-py",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}