{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TileDB VCF Example for 1000 Genomes Project\n",
    "\n",
    "This document is an example walkthrough of [TileDB-VCF](https://github.com/TileDB-Inc/TileDB-VCF.git) using Phase 3 of the [1000 Genomes Project](https://www.internationalgenome.org/) data. The goal is to highlight storing these samples in TileDB-VCF along with querying in python and exporting back to VCF and TSV.\n",
    "\n",
    "This expands upon the with specific use case. Please see the [official documentation](https://docs.tiledb.com/genomics/) for details on installing TileDB-VCF and more advanced usage.\n",
    "\n",
    "## Preprocessing the Raw 1KG pVCF File\n",
    "\n",
    "The original VCF file was downloaded from the AWS Open Data Registry: <a href=\"https://registry.opendata.aws/1000-genomes\" class=\"uri\">https://registry.opendata.aws/1000-genomes</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/bin/bash: aws: command not found\n"
     ]
    }
   ],
   "source": [
    "!aws s3 sync \\\n",
    "    --exclude \"*\" --include \"ALL.chr1.*\" \\\n",
    "    s3://1000genomes/release/20130502/ \\\n",
    "    data/1000genomes/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the more modern high-coverage version of the thousand genomes (1KG) data, which provides the raw single-sample gVCF files TileDB-VCF was designed for, the low-coverage Phase 3 data provides only cohort-level project VCF (pVCF) files for each contig. Therefore, we must first split the pVCF file back into single-sample VCF files prior to ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bcftools +split \\\n",
    "    -Ob \\\n",
    "    -o data/split-bcfs \\\n",
    "    data/1000genomes/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we’ll filter the split VCF files to include only records with a non-reference allele and remove `INFO` attributes that are either static (e.g., `NS`) or cohort-specific and recoverable (e.g., `AF`). We’ll save the final pre-processed files in *BCF* format, which is a binary representation of the VCF format. The resulting filtered BCF files are a close approximation of the raw single sample VCF files typically stored for large population genomics projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm_tags=INFO/AF,INFO/NS,INFO/EAS_AF,INFO/AMR_AF,INFO/AFR_AF,INFO/EUR_AF,INFO/SAS_AF\n",
    "\n",
    "!ls data/split-bcfs/*.bcf | parallel -j16 \\\n",
    "    \"bcftools view --min-ac 1 -Ou -s {/.} {} | bcftools annotate -Ob --remove $rm_tags -o data/filtered-bcfs/{/}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new BCF files must also be indexed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data/filtered-bcfs/*.bcf | parallel -j16 \"bcftools index {}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we’ll store these pre-processed files on S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive data/filtered-bcfs/ s3://genomic-datasets/notebooks/1kgp3/filtered-bcfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Data in TileDB VCF\n",
    "\n",
    "Next we will create a TileDB VCF dataset and ingest the VCF data.\n",
    "\n",
    "The following was run on a `m5.4xlarge` system with a 300GB EBS volume to handle the large number of VCF files.\n",
    "\n",
    "### Create the dataset\n",
    "\n",
    "You can create a TileDB VCF dataset anywhere that TileDB supports, this could be a local filesystem, s3, azure, gcs, hdfs, and more. For this example we’ll create it on s3, the most common use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the path below with your own s3 bucket\n",
    "!tiledbvcf create -u s3://genomic-datasets/notebooks/1kgp3/1kgp3-array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing the data\n",
    "\n",
    "To store the VCF data in TileDB VCF you simply need a list of the VCF/BCF file locations (e.g., local file paths, S3 URIs, *etc*).\n",
    "\n",
    "Run the following command to ingest the pre-processed BCF files into the TileDB-VCF dataset."
   ]
  },
  {
   "source": [
    "!tiledbvcf store -u s3://genomic-datasets/notebooks/1kgp3/1kgp3-array \\\n",
    "    --threads 16 \\\n",
    "    --sample-batch-size 20 \\\n",
    "    --verbose \\\n",
    "    --stats \\\n",
    "    data/filtered-bcfs/*.bcf"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running with `--verbose` you will get a summary printed at the end: `Done. Ingested 857,389,804 records (+ 4,076,523 anchors) from 2,504 samples in 2,325.25 seconds.`\n",
    "\n",
    "This indicates we’ve ingested over 857 million genomic positions into the TileDB VCF dataset. With the `m5.4xlarge` instance costing \\$0.768 an hour, and this ingestion took just under 40 minutes, the cost of ingestion was \\$0.50 USD.\n",
    "\n",
    "The final array is 7.7Gb, or just under half the size of the individual compressed BCF files.\n",
    "\n",
    "Following ingestion, it may help performance to consolidate the metadata fragments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$tiledbvcf utils consolidate fragment_meta -u s3://genomic-datasets/notebooks/1kgp3/1kgp3-array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading, Analysis and Exporting\n",
    "\n",
    "In this section we will walk through accessing the TileDB-VCF dataset with python and also exporting back to VCF and TSV.\n",
    "\n",
    "### Python API\n",
    "\n",
    "TileDB-VCF offers several APIs, for this section we will focus on the Python API. First we load the module and setup a few variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiledbvcf\n",
    "\n",
    "uri = \"s3://genomic-datasets/notebooks/1kgp3/1kgp3-array\"\n",
    "bedfile = \"s3://genomic-datasets/notebooks/1kgp3/hg37_chr1_covidHgiGwasR4PvalC2_plog3.bed.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading into Pandas Dataframe\n",
    "\n",
    "Pandas is one of the most popular data science tools in python. TileDB VCF’s python API produces results directly into a pandas dataframe. This makes its easy to analyze the data and leverage any of pandas’ builtin algorithms.\n",
    "\n",
    "Let’s run a typical query on the 1kg TileDB-VCF dataset we created above. We’ll retrieve all variants that overlap the gene *MTOR* on chr 1 for sample HG00096, along with a few attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tiledbvcf.Dataset(uri, stats = True)\n",
    "\n",
    "ds.read(\n",
    "    attrs = [\"sample_name\", \"contig\", \"pos_start\", \"pos_end\", \"alleles\", \"fmt_GT\"], \n",
    "    regions = [\"1:43337848-43352772\"],\n",
    "    samples = [\"HG00096\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the `Dataset` object was created with `stats = True` you can print out a variety of useful information about the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.tiledb_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above query took only 1.21914 secs for a single sample. Running this same query across all 2,504 samples took only 15.1911 secs.\n",
    "\n",
    "For production-sized queries that encompass large portions of the genome it's more convenient to provide bed files with the query regions. Here, we'll use a bed file on s3 that contains 1,040 regions on chr1 that show at least a moderate association with with SARS-CoV-2 infection susceptibility (data obtained from the [COVID-19 Host Genetics Initiative](https://www.covid19hg.org/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tiledbvcf.Dataset(uri, verbose = True)\n",
    "\n",
    "df = ds.read(\n",
    "    attrs = [\"sample_name\", \"contig\", \"pos_start\", \"pos_end\", \"alleles\", \"info_DP\", \"fmt_GT\"], \n",
    "    samples = ds.samples()[:10],\n",
    "    bed_file = bedfile\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This query completed in 2.5 secs for 10 samples.\n",
    "\n",
    "#### Filter Example\n",
    "\n",
    "Using the pandas dataframe returned from TileDB-VCF we can apply additional filters. For instance if we wanted to filter the above result on read depth (`info_DP`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.info_DP.apply(lambda x: x[0] > 5000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python API supports a variety of advanced uses, batching, partitioning, dask and more. We are happy to follow-up with additional details beyond these initial examples.\n",
    "\n",
    "### CLI Exporting\n",
    "\n",
    "In addition to the python API it is also possible to export the dataset back into VCF format. This can be helpful in interoperating with legacy tools.\n",
    "\n",
    "#### Exporting to VCF\n",
    "\n",
    "When exporting to VCF you can specify any number of samples, and each will be exported to its own file in vcf, compressed vcf or bcf format depdning on what you set for `--output-format`.\n",
    "\n",
    "For example to export the entire sample for `HG00096`, `HG00097`, and `HG00099` you can run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tiledbvcf export --uri s3://genomic-datasets/notebooks/1kgp3/1kgp3-array \\\n",
    "    --output-format v \\\n",
    "    --sample-names HG00096,HG00097,HG00099 \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces 3 files: `HG00096.vcf`, `HG00097.vcf`, and `HG00099.vcf`.\n",
    "\n",
    "##### Filtering Exports\n",
    "\n",
    "You can also combine the use of regions (bed file or list of regions passed to cli) to filter the export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tiledbvcf export --uri s3://genomic-datasets/notebooks/1kgp3/1kgp3-array \\\n",
    "    --output-format v \\\n",
    "    --sample-names HG00096,HG00097,HG00099 \\\n",
    "    --regions-file s3://genomic-datasets/notebooks/1kgp3/hg37_chr1_covidHgiGwasR4PvalC2_plog3.bed.gz \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will also produce the 3 VCF files, like the previous export. However, these files are filtered for the same SARS-CoV-2 associated genomic regions specified in the bed file.\n",
    "\n",
    "\n",
    "#### Exporting to TSV\n",
    "\n",
    "For even more generic usecases you can export data to tab seperate files with the `--output-format t` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$tiledbvcf export --uri s3://genomic-datasets/notebooks/1kgp3/1kgp3-array \\\n",
    "    --output-format t \\\n",
    "    --tsv-fields CHR,POS,I:END,REF,ALT,S:GT,Q:POS,Q:END \\\n",
    "    --sample-names HG00096,HG00097,HG00099 \\\n",
    "    --regions-file s3://genomic-datasets/notebooks/1kgp3/hg37_chr1_covidHgiGwasR4PvalC2_plog3.bed.gz \\\n",
    "    --verbose --output-path sars-cov-2-associated-regions.tsv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('tiledbvcf-py': conda)",
   "metadata": {
    "interpreter": {
     "hash": "31ba5af413596af6738a327c8a634dbdbb4a75a541688500a2f353ca9ee62cf0"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}